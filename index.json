[
{
	"uri": "https://kliii18.github.io/AWS---workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How INRIX accelerates transportation planning with Amazon Bedrock The complexity of modern traffic management extends far beyond mere road monitoring, encompassing massive amounts of data collected worldwide from connected cars, mobile devices, roadway sensors, and major event monitoring systems. For transportation authorities managing urban, suburban, and rural traffic flow, the challenge lies in effectively processing and acting upon this vast network of information. The task requires balancing immediate operational needs, such as real-time traffic redirection during incidents, with strategic long-term planning for improved mobility and safety.\nTraditionally, analyzing these complex data patterns and producing actionable insights has been a resource-intensive process requiring extensive collaboration. With recent advances in generative AI, there is an opportunity to transform how we process, understand, and act upon transportation data, enabling more efficient and responsive traffic management systems.\nIn this post, we partnered with Amazon Web Services (AWS) customer INRIX to demonstrate how Amazon Bedrock can be used to determine the best countermeasures for specific city locations using rich transportation data and how such countermeasures can be automatically visualized in street view images. This approach allows for significant planning acceleration compared to traditional approaches using conceptual drawings.\nINRIX pioneered the use of GPS data from connected vehicles for transportation intelligence. For over 20 years, INRIX has been a leader for probe-based connected vehicle and device data and insights, powering automotive, enterprise, and public sector use cases. INRIX‚Äôs products range from tickerized datasets that inform investment decisions for the financial services sector to digital twins for the public rights-of-way in the cities of Philadelphia and San Francisco. INRIX was the first company to develop a crowd-sourced traffic network, and they continue to lead in real-time mobility operations.\nIn June 2024, the State of California‚Äôs Department of Transportation (Caltrans) selected INRIX for a proof of concept for a generative AI-powered solution to improve safety for vulnerable road users (VRUs). The problem statement sought to harness the combination of Caltrans‚Äô asset, crash, and points-of-interest (POI) data and INRIX‚Äôs 50 petabyte (PB) data lake to anticipate high-risk locations and quickly generate empirically validated safety measures to mitigate the potential for crashes. Trained on real-time and historical data and industry research and manuals, the solution provides a new systemic, safety-based methodology for risk assessment, location prioritization, and project implementation.\nSolution overview INRIX announced INRIX Compass in November 2023. INRIX Compass is an application that harnesses generative AI and INRIX‚Äôs 50 PB data lake to solve transportation challenges. This solution uses INRIX Compass countermeasures as the input, AWS serverless architecture, and Amazon Nova Canvas as the image visualizer. Key components include:\nCountermeasures generation: INRIX Compass generates the countermeasures for a selected location Amazon API Gateway and Amazon Elastic Kubernetes Service (Amazon EKS) manage API requests and responses Amazon Bedrock Knowledge Bases and Anthropic‚Äôs Claude Models provide Retrieval Augmented Generation (RAG) implementation Image visualization API Gateway and AWS Lambda process requests from API Gateway and Amazon Bedrock Amazon Bedrock with model access to Amazon Nova Canvas provide image generation and in-painting The following diagram shows the architecture of INRIX Compass.\nINRIX Compass for countermeasures By using INRIX Compass, users can ask natural language queries such as, Where are the top five locations with the highest risk for vulnerable road users? and Can you recommend a suite of proven safety countermeasures at each of these locations? Furthermore, users can probe deeper into the roadway characteristics that contribute to risk factors, and find similar locations in the roadway network that meet those conditions. Behind the scenes, Compass AI uses RAG and Amazon Bedrock powered foundation models (FMs) to query the roadway network to identify and prioritize locations with systemic risk factors and anomalous safety patterns. The solution provides prioritized recommendations for operational and design solutions and countermeasures based on industry knowledge.\nThe following image shows the interface of INRIX Compass.\nImage visualization for countermeasures The generation of countermeasure suggestions represents the initial phase in transportation planning. Image visualization requires the crucial next step of preparing conceptual drawings. This process has traditionally been time-consuming due to the involvement of multiple specialized teams, including:\nTransportation engineers who assess technical feasibility and safety standards Urban planners who verify alignment with city development goals Landscape architects who integrate environmental and aesthetic elements CAD or visualization specialists who create detailed technical drawings Safety analysts who evaluate the potential impact on road safety Public works departments who oversee implementation feasibility Traffic operations teams who assess impact on traffic flow and management These teams work collaboratively, creating and iteratively refining various visualizations based on feedback from urban designers and other stakeholders. Each iteration cycle typically involves multiple rounds of reviews, adjustments, and approvals, often extending the timeline significantly. The complexity is further amplified by city-specific rules and design requirements, which often necessitate significant customization. Additionally, local regulations, environmental considerations, and community feedback must be incorporated into the design process. Consequently, this lengthy and costly process frequently leads to delays in implementing safety countermeasures. To streamline this challenge, INRIX has pioneered an innovative approach to the visualization phase by using generative AI technology. This prototyped solution enables rapid iteration of conceptual drawings that can be efficiently reviewed by various teams, potentially reducing the design cycle from weeks to days. Moreover, the system incorporates a few-shot learning approach with reference images and carefully crafted prompts, allowing for seamless integration of city-specific requirements into the generated outputs. This approach not only accelerates the design process but also supports consistency across different projects while maintaining compliance with local standards.\nThe following image shows the congestion insights by INRIX Compass.\nAmazon Nova Canvas for conceptual visualizations INRIX developed and prototyped this solution using Amazon Nova models. Amazon Nova Canvas delivers advanced image processing through text-to-image generation and image-to-image transformation capabilities. The model provides sophisticated controls for adjusting color schemes and manipulating layouts to achieve desired visual outcomes. To promote responsible AI implementation, Amazon Nova Canvas incorporates built-in safety measures, including watermarking and content moderation systems.\nThe model supports a comprehensive range of image editing operations. These operations encompass basic image generation, object removal from existing images, object replacement within scenes, creation of image variations, and modification of image backgrounds. This versatility makes Amazon Nova Canvas suitable for a wide range of professional applications requiring sophisticated image editing.\nThe following sample images show an example of countermeasures visualization.\nIn-painting implementation in Compass AI Amazon Nova Canvas integrates with INRIX Compass‚Äôs existing natural language analytics capabilities. The original Compass system generated text-based countermeasure recommendations based on:\nHistorical transportation data analysis Current environmental conditions User-specified requirements The INRIX Compass visualization feature specifically uses the image generation and in-painting capabilities of Amazon Nova Canvas. In-painting enables object replacement through two distinct approaches:\nA binary mask precisely defines the areas targeted for replacement. Text prompts identify objects for replacement, allowing the model to interpret and modify the specified elements while maintaining visual coherence with the surrounding image context. This functionality provides seamless integration of new elements while preserving the overall image composition and contextual relevance. The developed interface accommodates both image generation and in-painting approaches, providing comprehensive image editing capabilities. The implementation follows a two-stage process for visualizing transportation countermeasures. Initially, the system employs image generation functionality to create street-view representations corresponding to specific longitude and latitude coordinates where interventions are proposed. Following the initial image creation, the in-painting capability enables precise placement of countermeasures within the generated street view scene. This sequential approach provides accurate visualization of proposed modifications within the actual geographical context.\nAn Amazon Bedrock API facilitates image editing and generation through the Amazon Nova Canvas model. The responses contain the generated or modified images in base64 format, which can be decoded and processed for further use in the application. The generative AI capabilities of Amazon Bedrock enable rapid iteration and simultaneous visualization of multiple countermeasures within a single image. RAG implementation can further extend the pipeline‚Äôs capabilities by incorporating county-specific regulations, standardized design patterns, and contextual requirements. The integration of these technologies significantly streamlines the countermeasure deployment workflow. Traditional manual visualization processes that previously required extensive time and resources can now be executed efficiently through automated generation and modification. This automation delivers substantial improvements in both time-to-deployment and cost-effectiveness.\nConclusion The partnership between INRIX and AWS showcases the transformative potential of AI in solving complex transportation challenges. By using Amazon Bedrock FMs, INRIX has turned their massive 50 PB data lake into actionable insights through effective visualization solutions. This post highlighted a single specific transportation use case, but Amazon Bedrock and Amazon Nova power a wide spectrum of applications, from text generation to video creation. The combination of extensive data and advanced AI capabilities continues to pave the way for smarter, more efficient transportation systems worldwide.\nFor more information, check out the documentation for Amazon Nova Foundation Models, Amazon Bedrock, and INRIX Compass.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Yggdrasil Gaming boosts speed, resilience, and innovation with GOStack and AWS Global online casino games developer and publisher Yggdrasil Gaming set out to overcome the limitations of its legacy infrastructure and position itself for an AI-driven future. It partnered with software engineering provider GOStack and Amazon Web Services (AWS). The result: a high-performance, scalable platform that reduced infrastructure costs by 30%, increased processing capabilities sixfold, and improved uptime to 99.5%.\n‚ÄúTime to market is one of the main challenges for online gaming companies,‚Äù said Alex Haywood, Managing Director of Yggdrasil Gaming. ‚ÄúAWS gives us the power to develop quickly, adapt to different market conditions, and scale in new areas. We‚Äôve improved platform performance in nearly every aspect.‚Äù\nScaling beyond limits Founded in 2013 and headquartered in Malta, Yggdrasil was cloud-native from the beginning‚Äîbut as global demand surged, its original architecture could no longer keep up. Frequent disruptions led to degraded player experiences and lost revenue. In 2024, Yggdrasil teamed up with GOStack to completely reimagine its infrastructure using modern, cloud-native technologies on AWS.\n‚ÄúHigh availability is crucial for our operations‚Äîgameplay interruptions are unacceptable,‚Äù said Haywood. ‚ÄúBy moving to AWS with GOStack‚Äôs guidance, we‚Äôve reduced incident rates by 85% and virtually eliminated downtime.‚Äù\nGOStack: the driving force behind the transformation GOStack brought deep expertise in cloud and data architecture, DevOps, and scalable infrastructure. Acting as an extension of Yggdrasil‚Äôs engineering team, GOStack delivered a robust, cloud-native setup in just eight months, making sure of business continuity throughout the process.\nTheir contributions included the following:\nRedesigning the architecture using Amazon Elastic Kubernetes Service (Amazon EKS) for container orchestration and horizontal scaling Implementing Amazon Aurora for highly available, cost-efficient database management Deploying Amazon Elastic Compute Cloud (Amazon EC2) GPU instances to accelerate AI training and data processing Optimizing storage and content delivery with Amazon S3 and Amazon CloudFront Preparing for the future with Amazon Bedrock to enable generative AI capabilities Embracing managed services and automation allowed GOStack to help Yggdrasil streamline operations and free engineers to focus on innovation instead of maintenance.\nAn AI-driven future: no-code game creation for the next generation Yggdrasil is reimagining game development with a bold vision: eliminate technical barriers and empower creativity through AI and no-code tools. Central to this transformation is Game-in-a-Box, a fully managed no-code platform that enables designers, storytellers, and studios to build and launch games‚Äîwithout writing a single line of code.\n‚ÄúGame-in-a-Box gives creative people the superpowers they need,‚Äù said Haywood. ‚ÄúWe‚Äôre enabling a new generation of game creators who may not come from engineering backgrounds but have extraordinary ideas worth building.‚Äù\nGame-in-a-Box offers a modular environment where users can visually design mechanics, apply logic templates, upload assets, and configure distribution all through an intuitive interface. Generative AI assists with asset adaptation, localization, and QA, dramatically reducing time-to-market and cost.\nAbstracting backend complexity such as compliance, multiplayer scaling, and content delivery means that Game-in-a-Box lets creators focus on storytelling, gameplay, and player engagement. It runs securely and seamlessly on Yggdrasil‚Äôs modern AWS-powered infrastructure.\nLeaning into the power of data For Yggdrasil, data is more than just analytics: it‚Äôs a creative compass. Although data has long informed its strategy, the shift to AWS allowed Yggdrasil to uncover deeper, real-time insights into player behavior, game performance, and market trends.\nThe company‚Äôs new data infrastructure, built on AWS, features a sophisticated observability layer that captures every facet of player interaction‚Äîfrom session duration and drop-off points to bonus engagement and mechanic effectiveness. These insights now inform everything from game tuning to feature design.\n‚ÄúWe make our decisions based on all the available data about how the games are performing,‚Äù said Haywood. ‚ÄúWhether it‚Äôs identifying which mechanics resonate in a specific market or finding where players tend to drop off, we use these insights to improve our games and we share them with our partner studios so they can do the same.‚Äù\nTools such as Amazon Redshift, Amazon S3, and Amazon QuickSight allow Yggdrasil to deliver comprehensive, actionable dashboards across its ecosystem.\n‚ÄúWith this data foundation in place, we‚Äôre not just guessing‚Äîwe‚Äôre optimizing, adapting, and winning faster,‚Äù Haywood added.\nOngoing innovation Modernizing its infrastructure on AWS allowed Yggdrasil to unlock a new level of agility, performance, and scalability‚Äîlaying the foundation for continuous innovation. As global traffic grows, the company now delivers immersive content faster, with lower latency and near-zero downtime.\n‚ÄúAWS enables us to compete on a global scale,‚Äù said Haywood. ‚ÄúWe can now deliver content more efficiently and without delays and, more importantly, we can develop and launch new games faster than ever before.‚Äù\nWith a robust, AI-ready architecture in place, Yggdrasil is preparing for the next phase of its evolution where no-code tools, data intelligence, and automation fuel a more open and dynamic era of game development.\n‚ÄúThis isn‚Äôt just about scaling infrastructure‚Äîit‚Äôs about scaling imagination,‚Äù Haywood concluded.\nLearn more about developing games with AWS, or get in touch with an AWS for Games expert.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Qwen3 family of reasoning models now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart Today, we are excited to announce that Qwen3, the latest generation of large language models (LLMs) in the Qwen family, is available through Amazon Bedrock Marketplace and Amazon SageMaker JumpStart. With this launch, you can deploy the Qwen3 models‚Äîavailable in 0.6B, 4B, 8B, and 32B parameter sizes‚Äîto build, experiment, and responsibly scale your generative AI applications on AWS.\nIn this post, we demonstrate how to get started with Qwen3 on Amazon Bedrock Marketplace and SageMaker JumpStart. You can follow similar steps to deploy the distilled versions of the models as well.\nSolution overview Qwen3 is the latest generation of LLMs in the Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUnique support of seamless switching between thinking mode and non-thinking mode within a single model, providing optimal performance across various scenarios. Significantly enhanced in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Good human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open source models in complex agent-based tasks. Support for over 100 languages and dialects with strong capabilities for multilingual instruction following and translation. Prerequisites To deploy Qwen3 models, make sure you have access to the recommended instance types based on the model size. You can find these instance recommendations on Amazon Bedrock Marketplace or the SageMaker JumpStart console. To verify you have the necessary resources, complete the following steps:\nOpen the Service Quotas console. Under AWS Services, select Amazon SageMaker. Check that you have sufficient quota for the required instance type for endpoint deployment. Make sure at least one of these instance types is available in your target AWS Region. If needed, request a quota increase and contact your AWS account team for support.\nDeploy Qwen3 in Amazon Bedrock Marketplace Amazon Bedrock Marketplace gives you access to over 100 popular, emerging, and specialized foundation models (FMs) through Amazon Bedrock. To access Qwen3 in Amazon Bedrock, complete the following steps:\nOn the Amazon Bedrock console, in the navigation pane under Foundation models, choose Model catalog. Filter for Hugging Face as a provider and choose a Qwen3 model. For this example, we use the Qwen3-32B model. The model detail page provides essential information about the model‚Äôs capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.\nThe page also includes deployment options and licensing information to help you get started with Qwen3-32B in your applications.\nTo begin using Qwen3-32B, choose Deploy. You will be prompted to configure the deployment details for Qwen3-32B. The model ID will be pre-populated. 2. For Endpoint name, enter an endpoint name (between 1‚Äì50 alphanumeric characters). 3. For Number of instances, enter a number of instances (between 1‚Äì100). 4. For Instance type, choose your instance type. For optimal performance with Qwen3-32B, a GPU-based instance type like ml.g5-12xlarge is recommended. 5. To deploy the model, choose Deploy.\nWhen the deployment is complete, you can test Qwen3-32B‚Äôs capabilities directly in the Amazon Bedrock playground.\nChoose Open in playground to access an interactive interface where you can experiment with different prompts and adjust model parameters like temperature and maximum length. This is an excellent way to explore the model‚Äôs reasoning and text generation abilities before integrating it into your applications. The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results.You can quickly test the model in the playground through the UI. However, to invoke the deployed model programmatically with any Amazon Bedrock APIs, you must have the endpoint Amazon Resource Name (ARN).\nEnable reasoning and non-reasoning responses with Converse API The following code shows how to turn reasoning on and off with Qwen3 models using the Converse API, depending on your use case. By default, reasoning is left on for Qwen3 models, but you can streamline interactions by using the /no_think command within your prompt. When you add this to the end of your query, reasoning is turned off and the models will provide just the direct answer. This is particularly useful when you need quick information without explanations, are familiar with the topic, or want to maintain a faster conversational flow. At the time of writing, the Converse API doesn‚Äôt support tool use for Qwen3 models. Refer to the Invoke_Model API example later in this post to learn how to use reasoning and tools in the same completion.\nimport boto3 from botocore.exceptions import ClientError # Create a Bedrock Runtime client in the AWS Region you want to use. client = boto3.client(\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-west-2\u0026#34;) # Configuration model_id = \u0026#34;\u0026#34; # Replace with Bedrock Marketplace endpoint arn # Start a conversation with the user message. user_message = \u0026#34;hello, what is 1+1 /no_think\u0026#34; #remove /no_think to leave default reasoning on conversation = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: user_message}], } ] try: # Send the message to the model, using a basic inference configuration. response = client.converse( modelId=model_id, messages=conversation, inferenceConfig={\u0026#34;maxTokens\u0026#34;: 512, \u0026#34;temperature\u0026#34;: 0.5, \u0026#34;topP\u0026#34;: 0.9}, ) # Extract and print the response text. #response_text = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] #reasoning_content = response [\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;][\u0026#34;reasoning_content\u0026#34;][0][\u0026#34;text\u0026#34;] #print(response_text, reasoning_content) print(response) except (ClientError, Exception) as e: print(f\u0026#34;ERROR: Can\u0026#39;t invoke \u0026#39;{model_id}\u0026#39;. Reason: {e}\u0026#34;) exit(1) The following is a response using the Converse API, without default thinking:\n{\u0026#39;ResponseMetadata\u0026#39;: {\u0026#39;RequestId\u0026#39;: \u0026#39;f7f3953a-5747-4866-9075-fd4bd1cf49c4\u0026#39;, \u0026#39;HTTPStatusCode\u0026#39;: 200, \u0026#39;HTTPHeaders\u0026#39;: {\u0026#39;date\u0026#39;: \u0026#39;Tue, 17 Jun 2025 18:34:47 GMT\u0026#39;, \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;content-length\u0026#39;: \u0026#39;282\u0026#39;, \u0026#39;connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;x-amzn-requestid\u0026#39;: \u0026#39;f7f3953a-5747-4866-9075-fd4bd1cf49c4\u0026#39;}, \u0026#39;RetryAttempts\u0026#39;: 0}, \u0026#39;output\u0026#39;: {\u0026#39;message\u0026#39;: {\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: [{\u0026#39;text\u0026#39;: \u0026#39;\\n\\nHello! The result of 1 + 1 is **2**. üòä\u0026#39;}, {\u0026#39;reasoningContent\u0026#39;: {\u0026#39;reasoningText\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;\\n\\n\u0026#39;}}}]}}, \u0026#39;stopReason\u0026#39;: \u0026#39;end_turn\u0026#39;, \u0026#39;usage\u0026#39;: {\u0026#39;inputTokens\u0026#39;: 20, \u0026#39;outputTokens\u0026#39;: 22, \u0026#39;totalTokens\u0026#39;: 42}, \u0026#39;metrics\u0026#39;: {\u0026#39;latencyMs\u0026#39;: 1125}} The following is an example with default thinking on; the tokens are automatically parsed into the reasoningContent field for the Converse API:\n{\u0026#39;ResponseMetadata\u0026#39;: {\u0026#39;RequestId\u0026#39;: \u0026#39;b6d2ebbe-89da-4edc-9a3a-7cb3e7ecf066\u0026#39;, \u0026#39;HTTPStatusCode\u0026#39;: 200, \u0026#39;HTTPHeaders\u0026#39;: {\u0026#39;date\u0026#39;: \u0026#39;Tue, 17 Jun 2025 18:32:28 GMT\u0026#39;, \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;content-length\u0026#39;: \u0026#39;1019\u0026#39;, \u0026#39;connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;x-amzn-requestid\u0026#39;: \u0026#39;b6d2ebbe-89da-4edc-9a3a-7cb3e7ecf066\u0026#39;}, \u0026#39;RetryAttempts\u0026#39;: 0}, \u0026#39;output\u0026#39;: {\u0026#39;message\u0026#39;: {\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: [{\u0026#39;text\u0026#39;: \u0026#39;\\n\\nHello! The sum of 1 + 1 is **2**. Let me know if you have any other questions or need further clarification! üòä\u0026#39;}, {\u0026#39;reasoningContent\u0026#39;: {\u0026#39;reasoningText\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;\\nOkay, the user asked \u0026#34;hello, what is 1+1\u0026#34;. Let me start by acknowledging their greeting. They might just be testing the water or actually need help with a basic math problem. Since it\\\u0026#39;s 1+1, it\\\u0026#39;s a very simple question, but I should make sure to answer clearly. Maybe they\\\u0026#39;re a child learning math for the first time, or someone who\\\u0026#39;s not confident in their math skills. I should provide the answer in a friendly and encouraging way. Let me confirm that 1+1 equals 2, and maybe add a brief explanation to reinforce their understanding. I can also offer further assistance in case they have more questions. Keeping it conversational and approachable is key here.\\n\u0026#39;}}}]}}, \u0026#39;stopReason\u0026#39;: \u0026#39;end_turn\u0026#39;, \u0026#39;usage\u0026#39;: {\u0026#39;inputTokens\u0026#39;: 16, \u0026#39;outputTokens\u0026#39;: 182, \u0026#39;totalTokens\u0026#39;: 198}, \u0026#39;metrics\u0026#39;: {\u0026#39;latencyMs\u0026#39;: 7805}} Perform reasoning and function calls in the same completion using the Invoke_Model API With Qwen3, you can stream an explicit trace and the exact JSON tool call in the same completion. Up until now, reasoning models have forced the choice to either show the chain of thought or call tools deterministically. The following code shows an example:\nmessages = json.dumps( { \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi! How are you doing today?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;m doing well! How can I help you?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Can you tell me what the temperate will be in Dallas, in fahrenheit?\u0026#34; } ], \u0026#34;tools\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city to find the weather for, e.g. \u0026#39;San Francisco\u0026#39;\u0026#34; }, \u0026#34;state\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;the two-letter abbreviation for the state that the city is in, e.g. \u0026#39;CA\u0026#39; which would mean \u0026#39;California\u0026#39;\u0026#34; }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The unit to fetch the temperature in\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;city\u0026#34;, \u0026#34;state\u0026#34;, \u0026#34;unit\u0026#34;] } } }], \u0026#34;tool_choice\u0026#34;: \u0026#34;auto\u0026#34; }) response = client.invoke_model( modelId=model_id, body=body ) print(response) model_output = json.loads(response[\u0026#39;body\u0026#39;].read()) print(json.dumps(model_output, indent=2)) Response:\n{\u0026#39;ResponseMetadata\u0026#39;: {\u0026#39;RequestId\u0026#39;: \u0026#39;5da8365d-f4bf-411d-a783-d85eb3966542\u0026#39;, \u0026#39;HTTPStatusCode\u0026#39;: 200, \u0026#39;HTTPHeaders\u0026#39;: {\u0026#39;date\u0026#39;: \u0026#39;Tue, 17 Jun 2025 18:57:38 GMT\u0026#39;, \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;content-length\u0026#39;: \u0026#39;1148\u0026#39;, \u0026#39;connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;x-amzn-requestid\u0026#39;: \u0026#39;5da8365d-f4bf-411d-a783-d85eb3966542\u0026#39;, \u0026#39;x-amzn-bedrock-invocation-latency\u0026#39;: \u0026#39;6396\u0026#39;, \u0026#39;x-amzn-bedrock-output-token-count\u0026#39;: \u0026#39;148\u0026#39;, \u0026#39;x-amzn-bedrock-input-token-count\u0026#39;: \u0026#39;198\u0026#39;}, \u0026#39;RetryAttempts\u0026#39;: 0}, \u0026#39;contentType\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;body\u0026#39;: \u0026lt;botocore.response.StreamingBody object at 0x7f7d4a598dc0\u0026gt;} { \u0026#34;id\u0026#34;: \u0026#34;chatcmpl-bc60b482436542978d233b13dc347634\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;chat.completion\u0026#34;, \u0026#34;created\u0026#34;: 1750186651, \u0026#34;model\u0026#34;: \u0026#34;lmi\u0026#34;, \u0026#34;choices\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;reasoning_content\u0026#34;: \u0026#34;\\nOkay, the user is asking about the weather in San Francisco. Let me check the tools available. There\u0026#39;s a get_weather function that requires location and unit. The user didn\u0026#39;t specify the unit, so I should ask them if they want Celsius or Fahrenheit. Alternatively, maybe I can assume a default, but since the function requires it, I need to include it. I\u0026#39;ll have to prompt the user for the unit they prefer.\\n\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\\n\\nThe user hasn\u0026#39;t specified whether they want the temperature in Celsius or Fahrenheit. I need to ask them to clarify which unit they prefer.\\n\\n\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;chatcmpl-tool-fb2f93f691ed4d8ba94cadc52b57414e\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;arguments\u0026#34;: \u0026#34;{\\\u0026#34;location\\\u0026#34;: \\\u0026#34;San Francisco, CA\\\u0026#34;, \\\u0026#34;unit\\\u0026#34;: \\\u0026#34;celsius\\\u0026#34;}\u0026#34; } } ] }, \u0026#34;logprobs\u0026#34;: null, \u0026#34;finish_reason\u0026#34;: \u0026#34;tool_calls\u0026#34;, \u0026#34;stop_reason\u0026#34;: null } ], \u0026#34;usage\u0026#34;: { \u0026#34;prompt_tokens\u0026#34;: 198, \u0026#34;total_tokens\u0026#34;: 346, \u0026#34;completion_tokens\u0026#34;: 148, \u0026#34;prompt_tokens_details\u0026#34;: null }, \u0026#34;prompt_logprobs\u0026#34;: null } Deploy Qwen3-32B with SageMaker JumpStart SageMaker JumpStart is a machine learning (ML) hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. With SageMaker JumpStart, you can customize pre-trained models to your use case, with your data, and deploy them into production using either the UI or SDK.Deploying the Qwen3-32B model through SageMaker JumpStart offers two convenient approaches: using the intuitive SageMaker JumpStart UI or implementing programmatically through the SageMaker Python SDK. Let‚Äôs explore both methods to help you choose the approach that best suits your needs.\nDeploy Qwen3-32B through SageMaker JumpStart UI Complete the following steps to deploy Qwen3-32B using SageMaker JumpStart:\nOn the SageMaker console, choose Studio in the navigation pane. First-time users will be prompted to create a domain. On the SageMaker Studio console, choose JumpStart in the navigation pane. The model browser displays available models, with details like the provider name and model capabilities.\nSearch for Qwen3 to view the Qwen3-32B model card. Each model card shows key information, including:\nModel name Provider name Task category (for example, Text Generation) Bedrock Ready badge (if applicable), indicating that this model can be registered with Amazon Bedrock, so you can use Amazon Bedrock APIs to invoke the model Choose the model card to view the model details page. The model details page includes the following information:\nThe model name and provider information A Deploy button to deploy the model About and Notebooks tabs with detailed information The About tab includes important details, such as:\nModel description License information Technical specifications Usage guidelines Before you deploy the model, it‚Äôs recommended to review the model details and license terms to confirm compatibility with your use case. 6. Choose Deploy to proceed with deployment. 7. For Endpoint name, use the automatically generated name or create a custom one. 8. For Instance type, choose an instance type (default: ml.g6-12xlarge). 9. For Initial instance count, enter the number of instances (default: 1).\nSelecting appropriate instance types and counts is crucial for cost and performance optimization. Monitor your deployment to adjust these settings as needed. Under Inference type, Real-time inference is selected by default. This is optimized for sustained traffic and low latency. 10. Review all configurations for accuracy. For this model, we strongly recommend adhering to SageMaker JumpStart default settings and making sure that network isolation remains in place. 11. Choose Deploy to deploy the model.\nThe deployment process can take several minutes to complete.\nWhen deployment is complete, your endpoint status will change to InService. At this point, the model is ready to accept inference requests through the endpoint. You can monitor the deployment progress on the SageMaker console Endpoints page, which will display relevant metrics and status information. When the deployment is complete, you can invoke the model using a SageMaker runtime client and integrate it with your applications.\nDeploy Qwen3-32B using the SageMaker Python SDK To get started with Qwen3-32B using the SageMaker Python SDK, you must install the SageMaker Python SDK and make sure you have the necessary AWS permissions and environment set up. The following is a step-by-step code example that demonstrates how to deploy and use Qwen3-32B for inference programmatically:\n!pip install --force-reinstall --no-cache-dir sagemaker==2.235.2 from sagemaker.serve.builder.model_builder import ModelBuilder from sagemaker.serve.builder.schema_builder import SchemaBuilder from sagemaker.jumpstart.model import ModelAccessConfig from sagemaker.session import Session import logging sagemaker_session = Session() artifacts_bucket_name = sagemaker_session.default_bucket() execution_role_arn = sagemaker_session.get_caller_identity_arn() # Changed to Qwen32B model js_model_id = \u0026#34;huggingface-reasoning-qwen3-32b\u0026#34; gpu_instance_type = \u0026#34;ml.g5.12xlarge\u0026#34; response = \u0026#34;Hello, I\u0026#39;m a language model, and I\u0026#39;m here to help you with your English.\u0026#34; sample_input = { \u0026#34;inputs\u0026#34;: \u0026#34;Hello, I\u0026#39;m a language model,\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;max_new_tokens\u0026#34;: 128, \u0026#34;top_p\u0026#34;: 0.9, \u0026#34;temperature\u0026#34;: 0.6 } } sample_output = [{\u0026#34;generated_text\u0026#34;: response}] schema_builder = SchemaBuilder(sample_input, sample_output) model_builder = ModelBuilder( model=js_model_id, schema_builder=schema_builder, sagemaker_session=sagemaker_session, role_arn=execution_role_arn, log_level=logging.ERROR ) model = model_builder.build() predictor = model.deploy( model_access_configs={js_model_id: ModelAccessConfig(accept_eula=True)}, accept_eula=True ) predictor.predict(sample_input) You can run additional requests against the predictor:\nnew_input = { \u0026#34;inputs\u0026#34;: \u0026#34;What is Amazon doing in Generative AI?\u0026#34;, \u0026#34;parameters\u0026#34;: {\u0026#34;max_new_tokens\u0026#34;: 64, \u0026#34;top_p\u0026#34;: 0.8, \u0026#34;temperature\u0026#34;: 0.7}, } prediction = predictor.predict(new_input) print(prediction) The following are some error handling and best practices to enhance deployment code:\n# Enhanced deployment code with error handling import backoff import botocore import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) @backoff.on_exception(backoff.expo, (botocore.exceptions.ClientError,), max_tries=3) def deploy_model_with_retries(model_builder, model_id): try: model = model_builder.build() predictor = model.deploy( model_access_configs={model_id:ModelAccessConfig(accept_eula=True)}, accept_eula=True ) return predictor except Exception as e: logger.error(f\u0026#34;Deployment failed: {str(e)}\u0026#34;) raise def safe_predict(predictor, input_data): try: return predictor.predict(input_data) except Exception as e: logger.error(f\u0026#34;Prediction failed: {str(e)}\u0026#34;) return None Clean up To avoid unwanted charges, complete the steps in this section to clean up your resources.\nDelete the Amazon Bedrock Marketplace deployment If you deployed the model using Amazon Bedrock Marketplace, complete the following steps:\nOn the Amazon Bedrock console, under Foundation models in the navigation pane, choose Marketplace deployments. In the Managed deployments section, locate the endpoint you want to delete. Select the endpoint, and on the Actions menu, choose Delete. Verify the endpoint details to make sure you‚Äôre deleting the correct deployment: Endpoint name Model name Endpoint status Choose Delete to delete the endpoint. In the deletion confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint. Delete the SageMaker JumpStart predictor The SageMaker JumpStart model you deployed will incur costs if you leave it running. Use the following code to delete the endpoint if you want to stop incurring charges. For more details, see Delete Endpoints and Resources.\npredictor.delete_model() predictor.delete_endpoint() Conclusion In this post, we explored how you can access and deploy the Qwen3 models using Amazon Bedrock Marketplace and SageMaker JumpStart. With support for both the full parameter models and its distilled versions, you can choose the optimal model size for your specific use case. Visit SageMaker JumpStart in Amazon SageMaker Studio or Amazon Bedrock Marketplace to get started. For more information, refer to Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models, SageMaker JumpStart pretrained models, Amazon SageMaker JumpStart Foundation Models, Amazon Bedrock Marketplace, and Getting started with Amazon SageMaker JumpStart.\nThe Qwen3 family of LLMs offers exceptional versatility and performance, making it a valuable addition to the AWS foundation model offerings. Whether you‚Äôre building applications for content generation, analysis, or complex reasoning tasks, Qwen3‚Äôs advanced architecture and extensive context window make it a powerful choice for your generative AI needs.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders Purpose of the Event The event was organized to provide an in-depth perspective on the journey of digital transformation on the cloud computing platform in Vietnam. The main objectives include: AWS presenting its vision, strategy, and trends related to Cloud \u0026amp; GenAI for the Vietnamese market. Large enterprises such as Techcombank and U2U Network sharing real-world experiences in modernizing their systems. Workshops and discussion sessions offering practical knowledge for builders on Migration, Modernization, Security, and application development with GenAI. List of Speakers Eric Yeo ‚Äì Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner ‚Äì CEO, Techcombank Ms. Trang Phung ‚Äì CEO \u0026amp; Co-Founder, U2U Network Jaime Valles ‚Äì Vice President, General Manager Asia Pacific and Japan, AWS Jun Kai Loke ‚Äì AI/ML Specialist SA, AWS Kien Nguyen ‚Äì Solutions Architect, AWS Jun Kai Loke ‚Äì AI/ML Specialist SA, AWS Tamelly Lim ‚Äì Storage Specialist SA, AWS Binh Tran ‚Äì Senior Solutions Architect, AWS Taiki Dang ‚Äì Solutions Architect, AWS Michael Armentano ‚Äì Principal WW GTM Specialist, AWS Highlighted Content Opening Keynote: AWS introduced strategic directions in Vietnam and new opportunities for the builder generation. Real-world sharing: Techcombank and U2U Network presented lessons learned during their digital transformation journey. Panel ‚ÄúGenAI Revolution‚Äù: Leaders of many organizations discussed how to integrate AI into their development strategies. Migration \u0026amp; Modernization Track: Case study on migrating large workloads to AWS. Demonstration of Amazon Q Developer helping automate the software development process. Discussion about modernizing applications and security in the era of rapidly developing AI. Deep technical sessions: Roadmap for migrating VMware systems to AWS (EKS, RDS, serverless). Large-scale security models with analytical support from GenAI. What I Learned Every technical decision should begin with business needs. Effective migration should be divided into phases rather than shifting everything at once. Amazon Q Developer can significantly shorten the software development lifecycle. Security must be built into the design from the beginning, not added later. Application to Work Conduct event-storming with the team to identify suitable GenAI use cases. Try integrating Amazon Q Developer into the current development process. Apply a phased migration approach to upcoming cloud projects. Combine DevSecOps to ensure security throughout development and operation. Experience at the Event This was my first time attending Vietnam Cloud Day, and it truly left many impressions:\nListening directly to CEOs and CTOs helped me understand the bigger picture of digital transformation in Vietnam. The GenAI discussion panel was very practical ‚Äî each speaker had different viewpoints, but all emphasized the strong connection between AI and business strategy. The Amazon Q Developer demo helped me clearly visualize the power of GenAI across the entire SDLC. I had the opportunity to talk with many experts and builders, gaining many useful insights. Key Takeaways GenAI is powerful, but success still depends on people and strategy. Cloud transformation must have a clear roadmap ‚Äî rushing is risky. The combination of Cloud and AI will accelerate the growth of Vietnamese businesses. Builders need to continuously learn and be willing to experiment with new technologies. Some photos from the event "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AI-Driven Development Life Cycle: Reimagining Software Engineering Event Objectives The event aimed to clarify the evolving role of AI in modern software development, specifically:\nIntroducing emerging trends in AI-Driven Development ‚Äì an AI-supported and AI-orchestrated software development model. Presenting the AI-Driven Development Lifecycle (AI-DLC) framework, which integrates AI throughout the development process Demonstrating two representative tools: Amazon Q Developer and Kiro IDE Extension. Analyzing how AI enhances speed, productivity, and product quality for software development teams. Defining the future role of AI in development workflows. Speakers Lecturers: Toan Huynh \u0026amp; My Nguyen Facilitators: Diem My, Dai Truong, Dinh Nguyen Key Highlights Opening ‚Äì The Future of Software Development with AI Mr. Toan Huynh presented the topic ‚ÄúShaping the Future of Development,‚Äù describing the transition from traditional workflows to AI-Orchestrated Development, where AI coordinates various phases of product development ‚Äî from planning and design to coding and operations.\nLimitations of Current Development Models Current approaches such as AI-Assisted Development and AI-Managed Development still face limitations in stability, reliability, and explainability. Thus, AI-Driven Development (AI-DD) is introduced as a balanced approach: AI provides deep support, but humans remain the central decision-makers.\nAI-Driven Development Lifecycle (AI-DLC) Framework AI-DLC consolidates three stages of AI maturity in the software development process:\nAI-Assisted Development: AI suggests code, helps fix bugs, checks syntax. AI-Driven Development: AI contributes to architecture design, planning, and technical proposals. AI-Managed Development: AI orchestrates the entire workflow, while humans provide final approval. In this model, AI acts as an ‚Äúintelligent coordinator,‚Äù while developers validate and finalize results.\nBenefits of AI in Software Development The ‚ÄúAI in Development ‚Äì Outcomes‚Äù section highlights seven core values:\nPredictability ‚Äì improved stability and timeline forecasting. Velocity ‚Äì faster time-to-market. Quality ‚Äì fewer bugs and higher system reliability. Innovation ‚Äì more creative solution paths. Developer Engagement ‚Äì increased motivation and focus. Customer Satisfaction ‚Äì better user experience and service quality. Productivity ‚Äì optimized effort and time savings. SDLC Phases and AI‚Äôs Role According to the SDLC time distribution diagram, the process includes: Explore \u0026amp; Plan ‚Üí Create ‚Üí Test \u0026amp; Secure ‚Üí Review \u0026amp; Deploy ‚Üí Maintain, Transform \u0026amp; Modernize\nAI significantly reduces time spent in heavy phases such as testing, deployment, and maintenance through automation and smart analytics.\nStandard AI-DLC Workflow The standard AI-DLC model consists of four steps:\nRequirement ‚Äì The Product Owner analyzes and collects requirements. Design ‚Äì The system architect defines architecture, APIs, and workflows. Implementation ‚Äì Software engineers code, test, and integrate. Deployment ‚Äì System deployment and monitoring. AI accompanies all four steps to ensure clarity and alignment across roles.\nKey Features of the AI-DLC Workflow As described in ‚ÄúKey Workflow Features,‚Äù the process includes:\nRole Separation ‚Äì clear division of responsibilities between business, architecture, and development. AI-Enhanced ‚Äì AI generates role-specific contexts to provide optimal assistance. Iterative ‚Äì continuous feedback loops across all phases. Template-Driven ‚Äì standardized outputs using unified AIDLC templates. AI in Each Development Phase AI supports each step in three primary ways:\nSpecific Context ‚Äì AI works with tailored personas (PM, Dev, Architect). Clear Inputs/Outputs ‚Äì well-defined inputs and deliverables for each stage. Interactive Workflow ‚Äì enhanced two-way interaction between AI and humans. Documentation ‚Äì AI continuously updates all related documentation. Practical Demos: Amazon Q Developer \u0026amp; Kiro IDE Amazon Q Developer\nIntegrated directly into VS Code and Cloud9. Generates code, writes tests, creates documentation, proposes AWS architecture. Auto-updates prompt.md, suggests user stories, and supports CI/CD. The demo showcased AI-driven project planning and management via commands. Kiro IDE (presented by My Nguyen)\nA tool that generates specification documents (requirements.md, design.md, tasks.md). AI helps define APIs, generate features, and produce backend code. The demo demonstrated AI automatically generating a chat application with user authentication. Key Takeaways AI is a collaborator, not a replacement for developers. AI-DLC makes the development lifecycle clearer, more standardized, and easier to control. Amazon Q Developer significantly improves speed and reduces errors in SDLC. Kiro IDE demonstrates AI‚Äôs capability to support everything from requirements to code generation. DevSecOps combined with AI will become an essential industry standard. Real-World Applications Using Amazon Q Developer to automate testing and documentation in internal projects. Applying Kiro IDE to standardize specifications and accelerate backend development. Running an experimental ‚ÄúAI-Driven Sprint‚Äù to measure the impact of AI within the team. Implementing AI-Assisted Code Review to improve product quality. Personal Reflection The workshop ‚ÄúAI-Driven Development Workshop‚Äù provided many practical insights.\nMr. Toan Huynh‚Äôs presentation helped me better understand the strategic role AI will play in the future of software development, while Ms. My Nguyen‚Äôs demo with Kiro IDE demonstrated how AI can fully support the process from requirement analysis to code generation.\nThe event changed my perspective on AI: it is not merely a supporting tool, but a foundation that enables organizations to innovate faster and more sustainably.\nSome event photos "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4-3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Workshop \u0026ldquo;Data science on AWS‚Äù - Unlock the Power of Data with Cloud Computing Event Objectives Exploring the journey of building a modern Data Science system.\nSpeakers Van Hoang Kha ‚Äì Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong ‚Äì Cloud DevOps Engineer, AWS Community Builder Key Highlights Overview of AI and Foundational Concepts (by Mr. Kha) Mr. Kha began the session by systematizing the essential concepts in the field of artificial intelligence:\nArtificial Intelligence (AI): Described as a ‚Äúvirtual assistant,‚Äù AI allows machines to simulate human-like learning and analytical abilities. AI can process large volumes of data at high speed and continuously improve by learning from new data. Machine Learning (ML): A subset of AI where systems learn patterns from numerical or text data to make predictions or classifications. Deep Learning (DL): A deep neural network approach that enables machines to understand complex data types such as images, audio, and unstructured data. Generative AI: A type of AI model trained on massive datasets to generate new content such as text, images, or videos. Examples include Google‚Äôs Gemini and AWS‚Äôs Amazon Bedrock. This represents a major advancement over ML and DL because the model can ‚Äúcreate,‚Äù rather than just predict. AI/ML Service Layers on AWS (by Mr. Kha) Mr. Kha explained how AWS builds its AI ecosystem across three layers:\nLayer 1 ‚Äì AI Services\nReady-to-use services where users call APIs to solve specific problems such as computer vision, natural language processing, or text-to-speech.\nLayer 2 ‚Äì ML Services\nDesigned for users who need full control over data processing, model building, training, and fine-tuning. Amazon SageMaker is the central service, similar to Google Colab but more powerful and enterprise-ready.\nLayer 3 ‚Äì ML Frameworks \u0026amp; Infrastructure\nAWS provides compute infrastructure with GPU/CPU hardware in collaboration with companies like NVIDIA to support training very large models.\nTypical AI Services on AWS Several powerful AI services were showcased:\nAmazon Comprehend: Natural language processing and text information extraction Amazon Translate: Deep learning‚Äìbased translation Amazon Transcribe: Speech-to-text conversion Amazon Polly: Text-to-speech with natural-sounding voices Amazon Textract: Automated extraction from scanned documents and handwriting Amazon Rekognition: Face/object recognition and video analysis Amazon Personalize: Personalized recommendations like those used by Netflix or YouTube Machine Learning Model Building Workflow Mr. Kha shared the standard ML development lifecycle:\nFeature Engineering ‚Äì Clean, transform, and prepare data Model Training ‚Äì Teach the machine to learn patterns Evaluation \u0026amp; Tuning ‚Äì Test accuracy and iterate if needed Deployment ‚Äì Serve the model for real-world use Monitoring \u0026amp; Improvement ‚Äì Collect feedback and update the model AWS also introduced SageMaker Canvas, a no-code tool for building and deploying ML models with a simple drag-and-drop interface.\nHands-on Demo (by Mr. V∆∞∆°ng) Cleaning IMDb Data with AWS Glue\nRaw data stored in S3 AWS Glue automatically cleaned, processed, and normalized the data Processed data returned to S3, ready for training in SageMaker Cost comparison: Cloud vs. On-Premise\nCloud reduces upfront infrastructure and maintenance costs AWS allows scaling resources as needed, without hardware limits Experimenting with different models becomes faster and more flexible than with self-hosted systems Key Takeaways Strategy comes before technology: Digital transformation and GenAI adoption are most effective when driven by business needs‚Äînot trends. Migration requires a clear roadmap: Moving systems to the cloud should be done in phases to control risks and measure progress. GenAI accelerates software development: Through the Amazon Q Developer demo, I better understood how AI can automate many steps in the SDLC. Security must be built from the beginning: Security is not a ‚Äúpatching‚Äù stage‚Äîit must be embedded throughout the entire system design. People remain the decisive factor: No matter how advanced the technology is, success depends on mindset, learning ability, and execution. Practical Applications Conducting event storming sessions to identify AI use cases aligned with real business needs Integrating Amazon Q Developer into the current development workflow to measure improvements in coding, testing, and reviewing Planning phased cloud migration, starting with priority workloads to reduce risks Applying DevSecOps, ensuring end-to-end security from development to deployment Increasing internal discussions about AI/Cloud strategies for better team alignment Personal Reflections This event left a strong impression on me:\nThe GenAI discussion was highly practical, with varied viewpoints but a shared belief that AI must serve business goals. Watching the live demo of Amazon Q Developer gave me a clear vision of the future of AI-assisted software development. Networking with many experts and builders broadened my perspectives and provided valuable insights. Overall, the event was both inspirational and deeply practical, supporting my journey in Cloud and GenAI.\nSome event photos "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4-4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AI/ML/GenAI on AWS Workshop Event Objectives The workshop was designed to provide hands-on experience with AWS AI/ML services, focusing on Amazon SageMaker for traditional machine learning workflows and Amazon Bedrock for generative AI applications. The event aimed to help participants understand how to practically implement AI/ML solutions on AWS and explore the latest capabilities in generative AI.\nSpeakers Pham Nguyen Hai Anh - Cloud Engineer Program Overview 8:30 ‚Äì 9:00 | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaking activities to encourage collaboration Overview of the AI/ML ecosystem in Vietnam 9:00 ‚Äì 10:30 | Overview of AI/ML Services on AWS Amazon SageMaker ‚Äì A Comprehensive ML Platform\nData Preparation \u0026amp; Labeling: Including data cleaning, feature engineering, and automated labeling capabilities. Model Training, Fine-Tuning \u0026amp; Deployment: Exploring SageMaker training infrastructure, hyperparameter tuning, and deployment options (real-time \u0026amp; batch inference). Integrated MLOps Capabilities: Built-in model versioning, monitoring, and automated retraining pipelines. Live Demo: SageMaker Studio Walkthrough\nThe demonstration introduced the unified development environment for machine learning, including:\nIntegrated Jupyter notebook Experiment tracking and model registry Visual workflow builder for MLOps pipelines Integration with other AWS data processing services 10:30 ‚Äì 10:45 | Break Networking with refreshments and informal discussions on AI/ML use cases.\n10:45 ‚Äì 12:00 | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan ‚Äì Comparison \u0026amp; Selection Guide\nUnderstanding available foundation models on Bedrock Comparing capabilities, use cases, and performance characteristics Best practices for choosing the right model based on business requirements Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nCore Prompt Engineering Principles: Learn how to create effective prompts to obtain the desired outputs from language models Chain-of-Thought Reasoning: Understand how to guide the model through step-by-step reasoning processes to solve complex problems Few-shot Learning: A technique that provides examples to improve the model‚Äôs performance on specific tasks without requiring fine-tuning Retrieval-Augmented Generation (RAG)\nRAG architecture overview: Understand how RAG combines relevant information retrieval with generative capabilities. Knowledge Base Integration: Learn how to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Deployment Patterns: Best practices for building RAG applications that deliver accurate, context-aware responses Bedrock Agents: Multi-Step Workflows and Tool Integration\nAgent Architecture: Understand how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learn how to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications capable of handling complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understand Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learn how to configure custom content filters based on business requirements Compliance and Governance: Best practices to ensure AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot with Bedrock\nDemo section guiding the creation of a complete chatbot application:\nSet up the Bedrock foundation model Implement RAG with integrated knowledge base Configure Bedrock Agents for multi-turn conversations Add Guardrails for content safety Deploy the chatbot application Key Takeaways Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring. Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case. RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance. Production-Ready MLOps: SageMaker‚Äôs integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production. Safety as a Priority: Bedrock Guardrails ensure generative AI applications are safe, compliant, and aligned with business values. What I Learned SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity. Choosing the right foundation model is essential and depends on the specific use case, performance requirements, and cost constraints. Prompt engineering is a critical skill that can greatly improve model output without requiring fine-tuning. RAG architecture is necessary for building AI applications that need access to specific, up-to-date information. Bedrock Agents enable building sophisticated AI applications capable of handling complex multi-step workflows. Content safety must be considered from the very beginning when building generative AI applications. Real-World Applications Experimenting with SageMaker: Set up SageMaker Studio to explore ML model development for data analytics projects. Building RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation systems and Q\u0026amp;A. Practicing Prompt Engineering: Develop prompt engineering skills by creating templates and best practices for common use cases. Integrating MLOps: Apply SageMaker‚Äôs MLOps capabilities to automate training and model deployment pipelines. Deploying Safely: Integrate Bedrock Guardrails into any generative AI application to ensure content safety. Personal Impressions This workshop provides an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can optimize the entire ML workflow. Learning about RAG architecture was eye-opening, demonstrating how to build AI applications that leverage specific knowledge bases. The Bedrock Agents demonstration highlighted the potential for building sophisticated AI applications capable of complex workflows. The practical focus on prompt engineering provided immediately applicable skills for working with language models. Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications. Key Takeaways Start with the use case: Always begin by defining the specific business problem before choosing an AI/ML solution. Powerful foundation models: Pre-trained foundation models can solve many problems without custom training. RAG is essential: For applications requiring domain-specific knowledge, RAG architecture is the right approach. MLOps matters: Proper MLOps practices are crucial for maintaining ML models in production. Safety is non-negotiable: Content filtering and safety measures must be integrated from the start. Continuous learning: The AI/ML ecosystem evolves rapidly, requiring ongoing learning and experimentation. Some event photos "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4-5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "DevOps on AWS Workshop Event Objectives The workshop is designed to provide comprehensive knowledge and hands-on experience with AWS DevOps services, including CI/CD pipelines, Infrastructure as Code, container services, and monitoring \u0026amp; observability. The event aims to help participants understand DevOps culture, principles, and best practices, while also exploring how to practically implement DevOps processes on AWS.\nSpeakers Pham Nguyen Hai Anh - Cloud Engineer Danh Hoang Hieu Nghi - GenAI Engineer Program Overview Morning Session (8:30 ‚Äì 12:00) 8:30 ‚Äì 9:00 | Welcome \u0026amp; DevOps Mindset\nRecap of the AI/ML session from the previous workshop DevOps Culture and Principles: Understand the cultural shift from traditional IT to DevOps, emphasizing collaboration, automation, and continuous improvement Benefits and Key Metrics: DORA Metrics: Deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate MTTR (Mean Time To Recovery): Measures how quickly teams can recover from failures Deployment Frequency: Tracks how often teams deploy code to production Discussion on how DevOps practices improve software delivery and operational performance 9:00 ‚Äì 10:30 | AWS DevOps Services ‚Äì CI/CD Pipeline Source Control: AWS CodeCommit, Git Strategies\nAWS CodeCommit: A fully managed, secure Git-based source control service Git Strategies: GitFlow: Workflow using feature branches, develop, and release branches Trunk-based Development: Focus on the main branch with short-lived feature branches Best practices for choosing a branching strategy based on team size and project requirements Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines\nAWS CodeBuild: A fully managed build service that compiles source code, runs tests, and produces deployable artifacts Build Configuration: Buildspec files, environment variables, and build artifacts Testing Pipelines: Unit tests, integration tests, and automated test execution Integration with testing frameworks and code quality tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates\nAWS CodeDeploy: Automated application deployments to EC2, Lambda, or on-premises servers Blue/Green Deployment: Zero-downtime deployment using two identical production environments Canary Deployment: Gradual rollout to a small percentage of users before full deployment Rolling Updates: Incremental deployment across instances with automatic rollback capability Selecting the right deployment strategy based on application requirements Orchestration: Automating with CodePipeline\nAWS CodePipeline: A fully managed continuous delivery service for automating release pipelines Pipeline Stages: Source, Build, Test, Deploy, and Approval Integrations: Connect CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automation: Automated triggers, parallel actions, and pipeline visualization Demo: Full CI/CD Pipeline Walkthrough The demo showcases a complete CI/CD pipeline:\nSet up a CodeCommit repository Configure CodeBuild for automated builds and tests Create a CodeDeploy application with Blue/Green deployment Build a CodePipeline to orchestrate the entire workflow Test the pipeline with code changes and observe automated deployment 10:30 ‚Äì 10:45 | Break Networking and refreshments.\n10:45 ‚Äì 12:00 | Infrastructure as Code (IaC) AWS CloudFormation: Templates, Stacks v√† Drift Detection\nCloudFormation Templates: JSON/YAML templates defining AWS resources Stacks: Collections of AWS resources managed as a single unit Drift Detection: Detect changes made outside CloudFormation Stack Updates: Update infrastructure using change sets with rollback capability Best Practices: Template organization, parameterization, and nested stacks AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support\nAWS CDK: Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#, Go) Constructs: Reusable cloud components, from low-level resources to high-level patterns Reusable Patterns: Pre-built solutions for common use cases (VPC, ECS clusters, serverless applications) Language Support: TypeScript, Python, Java, C#, Go, and JavaScript Benefits: Type safety, IDE support, and easier testing compared to CloudFormation templates 10:30 ‚Äì 10:45 | Break Networking with refreshments and informal discussions on AI/ML use cases.\n10:45 ‚Äì 12:00 | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan ‚Äì Comparison \u0026amp; Selection Guide\nUnderstanding available foundation models on Bedrock Comparing capabilities, use cases, and performance characteristics Best practices for choosing the right model based on business requirements Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nCore Prompt Engineering Principles: Learn how to create effective prompts to obtain the desired outputs from language models Chain-of-Thought Reasoning: Understand how to guide the model through step-by-step reasoning processes to solve complex problems Few-shot Learning: A technique that provides examples to improve the model‚Äôs performance on specific tasks without requiring fine-tuning Retrieval-Augmented Generation (RAG)\nRAG architecture overview: Understand how RAG combines relevant information retrieval with generative capabilities. Knowledge Base Integration: Learn how to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Deployment Patterns: Best practices for building RAG applications that deliver accurate, context-aware responses Bedrock Agents: Multi-Step Workflows and Tool Integration\nAgent Architecture: Understand how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learn how to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications capable of handling complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understand Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learn how to configure custom content filters based on business requirements Compliance and Governance: Best practices to ensure AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot with Bedrock\nDemo section guiding the creation of a complete chatbot application:\nSet up the Bedrock foundation model Implement RAG with integrated knowledge base Configure Bedrock Agents for multi-turn conversations Add Guardrails for content safety Deploy the chatbot application Key Takeaways Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring. Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case. RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance. Production-Ready MLOps: SageMaker‚Äôs integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production. Safety as a Priority: Bedrock Guardrails ensure generative AI applications are safe, compliant, and aligned with business values. What I Learned SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity. Choosing the right foundation model is essential and depends on the specific use case, performance requirements, and cost constraints. Prompt engineering is a critical skill that can greatly improve model output without requiring fine-tuning. RAG architecture is necessary for building AI applications that need access to specific, up-to-date information. Bedrock Agents enable building sophisticated AI applications capable of handling complex multi-step workflows. Content safety must be considered from the very beginning when building generative AI applications. Real-World Applications Experimenting with SageMaker: Set up SageMaker Studio to explore ML model development for data analytics projects. Building RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation systems and Q\u0026amp;A. Practicing Prompt Engineering: Develop prompt engineering skills by creating templates and best practices for common use cases. Integrating MLOps: Apply SageMaker‚Äôs MLOps capabilities to automate training and model deployment pipelines. Deploying Safely: Integrate Bedrock Guardrails into any generative AI application to ensure content safety. Personal Impressions This workshop provides an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can optimize the entire ML workflow. Learning about RAG architecture was eye-opening, demonstrating how to build AI applications that leverage specific knowledge bases. The Bedrock Agents demonstration highlighted the potential for building sophisticated AI applications capable of complex workflows. The practical focus on prompt engineering provided immediately applicable skills for working with language models. Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications. Key Takeaways Start with the use case: Always begin by defining the specific business problem before choosing an AI/ML solution. Powerful foundation models: Pre-trained foundation models can solve many problems without custom training. RAG is essential: For applications requiring domain-specific knowledge, RAG architecture is the right approach. MLOps matters: Proper MLOps practices are crucial for maintaining ML models in production. Safety is non-negotiable: Content filtering and safety measures must be integrated from the start. Continuous learning: The AI/ML ecosystem evolves rapidly, requiring ongoing learning and experimentation. Some event photos "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Khanh Ly\nPhone Number: 0386357809\nEmail: lynkse181666@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn how to draw AWS architecture and practice on draw.io - Learn how to write Lab Guide and create Workshop 09/09/2025 09/09/2025 https://www.youtube.com/watch?v=l8isyDe-GwY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=2 https://drive.google.com/file/d/1UQD__1g2UNTcasWJaaVUTPsUM24ympE3/view?usp=sharing https://van-hoang-kha.github.io/vi/1-introduce/ 4 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Know what cloud computing is? How to work with AWS and how to optimize costs in practice - Practice: + Create new AWS account + MFA for AWS Accounts + Create Admin Group and Admin Use 11/09/2025 11/09/2025 https://000001.awsstudygroup.com/ 6 - Practice: + Create Budget by Template + Create Cost Budget + Create Usage Budget + Create Savings Plans Budget - Learn about AWS Support Packages 12/09/2025 12/09/2025 https://000007.awsstudygroup.com/vi/ https://000009.awsstudygroup.com/vi/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand and learn how AWS services operate. Gain a solid understanding of the Hybrid DNS mechanism in AWS using Route 53. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about some of the most important networking services on AWS: + Amazon Virtual Private Cloud (VPC) + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing - Practice: + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs 15/09/2025 15/09/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 https://000003.awsstudygroup.com/ 3 - Understand how to deploy a comprehensive EC2 infrastructure - Practice: + Create EC2 Server + Test Connection using MobaXterm + Create NAT Gateway + Create EC2 Instance Connect Endpoint + AWS System Manager Session Manager + Create VPC for VPN + Create EC2 as a Customer Gateway + Create Virtual Private Gateway + Create Customer Gateway + Cretae VPN Connection + Create Transit Gateway + Create + Create Transit Gateway Attachment + Clean up resources 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ 4 - Learn about Route 53 and understand the three tools used to enable a hybrid DNS architecture between on-premises DNS systems and AWS: + Outbound Endpoints + Inbound Endpoints + Route 53 Resolver Rules - Understand the concepts of AWS Quick Starts, AWS CloudFormation, and AWS Directory Service - Practice: + Generate Key Pair + Initialize CloudFormation Template + Configuring Security Group - Learn how to log in to the Remote Desktop Gateway (RDGW) server using the Remote Desktop Protocol (RDP) - Practice: + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results + Clean up resources 17/09/2025 17/09/2025 https://000010.awsstudygroup.com/ 5 - Set up a VPC Peering connection between two VPCs so that the resources within those VPCs can communicate directly with each other - Practice: + Initialize CloudFormation Template + Create Secuity Group + Create EC2 Instance + Update Network ACL + VPC Peering + Cross-Peer DNS + Cleanup 18/09/2025 18/09/2025 https://000019.awsstudygroup.com/ 6 - Practice: + Generate Key Pair + Initialize CloudFormation Template + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables + Clean up resources 19/09/2025 19/09/2025 https://000020.awsstudygroup.com/ Week 2 Achievements: Understand that VPC allows the creation of AWS resources such as virtual servers, databases, and load balancers within a private virtual network environment where users have full control.\nDistinguish between VPC Peering and Transit Gateway:\nVPC Peering is a feature that connects two or more VPCs so that resources within those VPCs can communicate directly without going through the Internet. Transit Gateway is used to connect multiple VPCs and on-premises networks through a centralized hub. Understand the Hybrid DNS model using Route 53 Resolver:\nOutbound Endpoints: Route 53 Resolver sends DNS queries from AWS to your on-premise DNS system through these endpoints. Inbound Endpoints: These endpoints serve as targets for DNS queries from your on-premise DNS system to domains hosted on AWS. Route 53 Resolver Rules: Utilizing Resolver Rules, you can configure Route 53 to forward DNS queries for specific domains to your on-premise DNS system. Successfully deployed a customized VPC, including public and private Subnets, Internet Gateway, Route Table, and Security Group.\nEstablished VPN Connection between the AWS VPC and a simulated on-premises network.\nSuccessfully implemented VPC Peering between two VPCs and verified resource connectivity.\nMastered the process of creating, running, and deleting resources to optimize costs and avoid unnecessary charges.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Learn and master Compute \u0026amp; Migration services on AWS. Get familiar and hands-on with AWS Backup. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore virtual server services on AWS + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon Elastin File System (EFS) + Amazon FSX + AWS Application Migration Service (MGN) - Get familiar with using AWS Backup to create a backup plan - Practice: + Create S3 Bucket + Deploy infrastructure + Create Backup plan - Get familiar with AWS CLI - Practice: + Test Restore + Clean up resources 22/09/2025 22/09/2025 https://000013.awsstudygroup.com/ 3 - Practice: + Deploy File Storage Gateway + Create Storage Gateway + Create File Shares + Mount File shares on On-premises machine + Clean up resources - Know the difference between S3 Bucket and S3 Object - Know the key features of S3: + Storage Class + Security \u0026amp; Compliance + Management \u0026amp; Analytics + Performance \u0026amp; Availability - Know the common S3 use cases: + Data Lakes \u0026amp; Analytics + Backup \u0026amp; Restore + Content Delivery + Data Archiving + Disaster Recovery + Application Data Archiving + Media \u0026amp; Entertainment - Know the benefits of using S3: + Scalability: Virtually unlimited storage, grow with your needs + Durability: 99.999999999% durability (11 9s) protects against data loss + Cost-Effective: Pay only for what you use with multiple pricing tiers + Security: Enterprise-grade security with multiple encryption options + Integration: Seamless integration with other AWS services + Global Access: Access your data from anywhere in the world 23/09/2025 23/09/2025 https://000024.awsstudygroup.com/ https://000057.awsstudygroup.com/1-introduce/ 4 - Practice: + Enable static website feature + Configuring public access block + Configuring public objects + Test website + Block all public access + Config Amazon CloudFront + Test Amazon Cloudfront + Move objects + Replication Object multi Region - Learn about S3 Versioning - Learn about Cross-Region Replication - Clean up resources 24/09/2025 24/09/2025 https://000057.awsstudygroup.com/ 5 - Research AWS storage services: + Amazon Simple Storage Service (S3) + Snow Family + AWS Storage Gateway + Disaster Recovery on AWS + AWS Backup - Get familiar with using AWS Backup to create a backup plan, understand how to restore data from backups, and automate the entire process - Practice: + Create Backup plan + Configure Block Public Access + Configure Public Object + Test Website 25/09/2025 25/09/2025 https://000013.awsstudygroup.com/ 6 - Practice: + Prepare an instance on the VMware Workstation virtualization environment + Migrate a virtual machine from VMware Workstation to the AWS platform + Upload the virtual machine to AWS + Import the virtual machine into AWS + Deploy an EC2 Instance from an AMI + Configure ACL for an S3 Bucket + Export the virtual machine from the EC2 Instance + Export the virtual machine from the AMI + Clean up resources 26/09/2025 26/09/2025 https://000014.awsstudygroup.com/ Week 3 Achievements: Understand the operation and characteristics of virtual server services:\nEC2 Lightsail EFS FSx Understand the differences between file storage services (EFS, FSx) and how they integrate with EC2.\nLearn how AWS MGN works and understand the process of migrating virtual machines from on-premise to AWS.\nBecome proficient in basic operations with AWS Backup.\nClearly distinguish between S3 Bucket and S3 Object.\nS3 Bucket A ‚Äúcontainer‚Äù used to store objects. Manages access permissions, policies, versioning, and replication. Each bucket has a unique name and belongs to a specific region. S3 Object Actual data stored inside a bucket (files, images, videos‚Ä¶). Includes: key (name), data (content), metadata. A bucket can contain an unlimited number of objects. Master key features of S3:\nStorage Classes Security \u0026amp; Compliance (IAM, encryption, Block Public Access) Management \u0026amp; Analytics Creating and managing key pairs Performance \u0026amp; Availability Understand common use cases: Data Lake, Backup, Disaster Recovery (DR), Content Distribution‚Ä¶\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand the core security services of AWS. Master AWS Storage Gateway \u0026amp; S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Create S3 Bucket + Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares + Mount File shares on On-premises machine + Clean up resources + Create new file shares + Enable data deduplication + Enable shadow copies 29/09/2025 29/09/2025 https://000024.awsstudygroup.com/ https://000025.awsstudygroup.com/ 3 - Learn about security services on AWS + Shared Responsibility Model + AWS Identity and Access Management + Amazon Cognito + AWS Organization + AWS Identity Center (SSO) + AWS Key Management Service (KMS) AWS Security Hub - Practice: + Enable Security Hub + Create VPC + Create Security Group + Create EC2 instance + Incoming Web-hooks slack + Create Tag for Instance + Create Role for Lambda + Create Lambda Function + Clean up resources 30/09/2025 30/09/2025 https://000018.awsstudygroup.com/ 4 - Practice: + Use tags on Console + Using tags with CLI + Create a Resource Group + Create IAM User + Create IAM Policy + Create IAM Role + Check IAM Policy 01/10/2025 01/10/2025 https://000027.awsstudygroup.com/ https://000028.awsstudygroup.com/ 5 - Learn about IAM Permission Boundary - Practice: + Create Restriction Policy + Create IAM Limited User + Test IAM User Limits + Create Policy and Role + Create Group and User + Create Key Management Service + Create Amazon S3 + Create AWS CloudTrail and Amazon Athena + Clean up resources 02/10/2025 02/10/2025 https://000030.awsstudygroup.com/ https://000033.awsstudygroup.com/ 6 - Review IAM concepts - Learn about requests, authentication of requests to AWS services, and the role-assumption process - Practice: + Create IAM Group + Create IAM User + Configure Role Condition + Clean up resources + Create EC2 Instance + Create S3 bucket + Generate IAM user and access key + Use access key + Create IAM role + Using IAM role + Clean up resources 03/10/2025 03/10/2025 https://000044.awsstudygroup.com/ https://000048.awsstudygroup.com/ Week 4 Achievements: Deploy a complete Storage Gateway system:\nCreate an S3 Bucket to serve as the storage backend. Create an EC2 instance to act as the virtual appliance for Storage Gateway. Configure and successfully activate the Storage Gateway. Create a File Share (NFS/SMB). Connect the File Share from the on-premises machine. Enable advanced features: Deduplication (eliminate duplicate data). Shadow Copies (restore file versions). Know how to deploy a monitoring and alerting system.\nKnow how to assign tags on the Console, assign tags using AWS CLI, and create Resource Groups based on tags.\nBe able to build a security monitoring system with Security Hub + Lambda + Slack.\nSet up professional logging \u0026amp; auditing (CloudTrail + Athena).\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Become proficient in working with NoSQL databases on AWS. Learn about Data Lake Architecture. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about database services on AWS: + Database + Session + Primary key + Foreign key + Index + Partitions + Execution Plan ‚Äì Query Plan + Database Log + Buffer + RDBMS + NOSQL + OLTP ‚Äì Online Transaction Processing + OLAP ‚Äì Online Analytical Processing + Amazon RDS \u0026amp; Aurora + Amazon Redshift + Amazon ElastiCache - Practice: + Create a VPC + Create EC2 Security Group + Create RDS Security Group + Create DB Subnet Group + Create EC2 instance + Create RDS database instance + Application Deployment + Backup and Restore + Clean up resources 06/10/2025 06/10/2025 https://000005.awsstudygroup.com/ 3 - Learn about Data Lake - Practice: + Creating an IAM Role + Create Policy + Create S3 Bucket + Creating a Delivery Stream + Create Sample Data + Create Data Catalog + Data Transformation + Analysis and visualization + Clean up resources 07/10/2025 07/10/2025 https://000035.awsstudygroup.com/ 4 - Learn about AWS Glue \u0026amp; Amazon Athena - Practice: + Preparing the database + Building a database + Database Check + Analysis of cost and usage performance + Clean up resources 08/10/2025 08/10/2025 https://000040.awsstudygroup.com/ 5 - Practice: + Create access key + Create a table + Write data + Read data + Update data + Query data + Create a Global Secondary Index + Query the Global Secondary Index + Clean up resource 09/10/2025 09/10/2025 https://000060.awsstudygroup.com/ 6 - Practice: + Creating a Cloud9 Instance + Download Dataset + Check encoding + Upload Dataset to S3 + Setting up DataBrew + Data Profiling + Clean \u0026amp; Tranform data + Preparing the Next Table + Upload cleaned dataset + Configuring roles for AWS Glue + Creating a Data Catalog + Transform to Parquet + Transform to Parquet-2 + Creating a New Data Catalog + Check schema information + Install Athena + Basic query + Join 2 table + Create table as select + Create view + Data Partition + Columnar vs Row based + Register for QuickSight + Configuring Permissions + Connect Dataset + Edit Dataset + Building a Dashboard + Resource Cleanup 10/10/2025 10/10/2025 https://000070.awsstudygroup.com/ Week 5 Achievements: Master foundational database concepts:\nUnderstand important concepts such as primary key, foreign key, session, index, partition, execution plan, database log, and buffer. Clearly distinguish between RDBMS and NoSQL, and understand the characteristics of each model. Successfully deploy and operate Amazon RDS.\nUnderstand Data Lake architecture and build your first data pipeline.\nAcquire in-depth skills in using AWS Glue \u0026amp; Amazon Athena.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Review important architectural designs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore AWS Billing Dashboard and Cost Explorer - View costs by service, region, and time range - Set up Cost Anomaly Detection - Create an AWS Budget and configure cost alert notifications via email - Write a weekly cost summary report and propose optimizations (turn off EC2, clean up EBS, reduce log retention, etc.) 13/10/2025 13/10/2025 3 - Learn the concepts of High Availability, Fault Tolerance, and Elasticity - Learn about Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) - Configure a launch template, scaling policy, and target tracking - Connect the ALB to the ASG for load distribution - Test website access through the ALB‚Äôs DNS name 14/10/2025 14/10/2025 4 - Overview of the AWS Well-Architected Framework and its 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization - Identify the role and importance of each pillar in system design - Review Secure Architecture Design ‚Üí IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 15/10/2025 15/10/2025 5 - Review Flexible and Resilient Architecture Design (Resilient Architectures) ‚Üí Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore - Review High-Performing \u0026amp; Cost-Optimized Architectures ‚Üí EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, Cost Explorer, Budgets, Savings Plans, Storage Tiering 16/10/2025 16/10/2025 6 - Practice: + Build a sample architecture combining EC2, S3, RDS, IAM, VPC, CloudFront, Lambda, and CloudWatch + Evaluate it based on the 5 pillars of the Well-Architected Framework 17/10/2025 17/10/2025 Week 6 Achievements: Know how to manage AWS costs.\nUnderstand and implement High Availability / Auto Scaling architectures.\nMaster the AWS Well-Architected Framework.\nBe able to design a standard, secure, reliable, cost-optimized, and scalable architecture.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand the Data \u0026amp; Analytics ecosystem on AWS. Gain an overview of AI/ML on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore the Data \u0026amp; Analytics ecosystem on AWS - Understand the concepts of Data Lake, ETL pipelines, and how to integrate data from multiple sources - Set up an AWS Glue Crawler to detect data schemas 20/10/2025 20/10/2025 3 - Learn an overview of AI/ML on AWS - Learn about ML-supporting services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly - Practice: + Create a Notebook Instance + Train a simple model (Linear Regression / Image Classification) + Deploy an endpoint and test predictions 21/10/2025 21/10/2025 4 - Practice Amazon Comprehend (natural language analysis) - Experiment with Amazon Kendra (context-aware intelligent search) - Compare the strengths and limitations of each service 22/10/2025 22/10/2025 5 - Study the concepts of Modernization and Serverless - Compare monolithic and microservices architectures - Ph√¢n t√≠ch c√°c l·ª£i √≠ch khi chuy·ªÉn ƒë·ªïi sang serverless - Practice with AWS Lambda: create a function, configure triggers, view logs in CloudWatch - Deploy basic API processing logic using Lambda 23/10/2025 23/10/2025 6 - Integrate API Gateway with Lambda to build a REST API - Connect data with DynamoDB (CRUD operations) - Test the API using Postman - Configure Cognito for user authentication (user pool, token) - Integrate Cognito authentication into API Gateway - Manage access permissions using IAM Roles 24/10/2025 24/10/2025 Week 7 Achievements: Build a strong foundation in Data \u0026amp; Analytics on AWS:\nFully understand Data Lakes, ETL pipelines, and Glue Crawlers. Be able to deploy processes for detecting and managing data schemas. Know how to use AWS AI/ML services.\nDeploy machine learning models on SageMaker.\nBe proficient in using AI Managed Services.\nHave a clear understanding of Serverless architecture and Cloud Modernization.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Focus on midterm exam preparation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Focus on reviewing theory to prepare for the midterm exam 27/10/2025 27/10/2025 3 Focus on reviewing theory to prepare for the midterm exam 28/10/2025 28/10/2025 4 Review previous hands-on exercises and practice them again on the AWS Console 29/10/2025 29/10/2025 5 Review previous hands-on exercises and practice them again on the AWS Console 30/10/2025 30/10/2025 6 Midterm exam 31/10/2025 31/10/2025 Week 8 Achievements: The exam result was 220 / 670 "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn about networking services on AWS\nWeek 3: Compute, Storage \u0026amp; Migration on AWS\nWeek 4: AWS Security, Storage Gateway \u0026amp; IAM\nWeek 5: Database Foundations, Data Lake, and AWS Analytics\nWeek 6: AWS Cost Management, High Availability \u0026amp; Well-Architected Design\nWeek 7: Data Analytics, AI/ML, and Serverless Modernization\nWeek 8: Midterm exam\nWeek 9: Aurora DSQL ‚Äì Serverless Distributed SQL\nWeek 10: Amazon Q Developer Plugins (Datadog \u0026amp; Wiz)\nWeek 11: Security, Monitoring \u0026amp; Issue Triage\nWeek 12: Summary \u0026amp; Review of Real-World Infrastructure Operations\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop Introduction This workshop guides you through building a serverless Excel Import system on AWS with authentication and data processing features.\nObjectives By the end of this workshop, we will:\nGet familiar with serverless architecture: How to design applications with Lambda, API Gateway, S3, and DynamoDB Get familiar with event-driven architecture: Use S3 Event Notifications to trigger automatic processing Implement authentication: Integrate Amazon Cognito for authentication and API security Parse Excel in Lambda: Use the Apache POI library to read and process .xlsx/.xls files Deploy with AWS SAM: Get familiar with the SAM CLI to build and deploy, and apply IaC for the entire infrastructure. Key Components 8 Lambda Functions: Register, Confirm, Login, Logout, GenerateUploadUrl, ListImportJobs, GetJobStatus, ImportS3Trigger 3 DynamoDB Tables: Students, Courses, ImportJobs 1 S3 Bucket: Store Excel files with a lifecycle policy (auto-delete after 7 days) 1 Cognito User Pool: Manage users and authentication 1 API Gateway: REST API with a Cognito authorizer Time \u0026amp; Cost Estimated completion time: ~30 minutes\nCost: Free (All within the Free Tier)\nTo avoid unexpected charges, perform cleanup immediately after finishing the workshop.\nRequirements Basic understanding of AWS (console, regions, basic services) Ability to use the terminal/command line Ability to read Java "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.4-deploy-backend/5.4.1-sam-build/",
	"title": "SAM Build",
	"tags": [],
	"description": "",
	"content": "Build Project with Maven Maven will compile the Java code and download all necessary dependencies.\nClean\ncd excel-import-workshop mvn clean Package application\nmvn package This process will:\nDownload dependencies Compile Java source code Package into a JAR file Build with AWS SAM The SAM CLI will prepare Lambda deployment packages.\nsam build This process will:\nRead template.yaml Find all Lambda functions Copy compiled code from target/ into .aws-sam/build/ Create deployment packages for each function "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Teaching Center Management System AWS Serverless Solution for Teaching Center Management Project 1. Executive Summary The project focuses on deploying an LMS (Learning Management System) platform serving core training operations, equivalent in scope to systems like lms-hcmuni.fpt.edu.vn. Specifically, the scope includes 2 main parts: academic management and authentication/identification and authorization.\nThe goal is to provide essential LMS capabilities: managing courses and classes; class schedules; searching and enrollment using enrollKey; role-based dashboards (Admin, Teacher, Student); managing instructor/student profiles; managing course documents on S3 (creating folders, uploading, downloading with enrollment verification); and batch data import from Excel to quickly initialize academic data. The authentication/identification and authorization part ensures login/authentication with Amazon Cognito (supporting OAuth/Google), invitation/redemption of invitations, password reset/change, token refresh, user profiles, and role-based authorization mechanisms to protect academic resources.\nIn the future, the system can gradually expand to other modules (CRM, payments, HRM, etc.) and consider integrating AI/IoT when needed, but these are not within the scope of the current deployment.\n2. Problem Statement Current Issues\nIn the LMS context, many training units operate in silos with discrete steps: creating courses/classes, publishing schedules, enrolling students, managing documents, and accessing profiles ‚Äî typically across multiple different tools. This leads to fragmented data, difficult access control, manual enrollment processes (prone to errors), and inconsistent user experience between instructors and students.\nAs the number of courses/classes increases, there\u0026rsquo;s a lack of unified role-based authorization mechanisms, reliable authentication (SSO/OAuth), and clear academic APIs to support role-based dashboards. The absence of batch data import channels also causes delays when initializing new semesters.\nSolution\nThe platform is deployed on AWS Serverless architecture, focusing on addressing two main pillars:\nAuthentication/Identification and Authorization: Amazon Cognito for authentication (email/password, Google OAuth), invitations (invite/redeem), password reset/change, token refresh, user profiles; role-based authorization (ADMIN/TEACHER/STUDENT) to protect APIs and academic resources. Academic Management: APIs for managing courses/classes, unified search, enrollment using enrollKey (activating PRE_ENROLLED ‚Üí ACTIVE), role-based dashboards, instructor/student profiles, and course document management on S3 (creating folders, presigned upload/download with enrollment verification). Supports Excel import for quick data initialization and full rollback on errors. The access flow: CloudFront ‚Üí S3 (static content) and API Gateway ‚Üí Lambda ‚Üí DynamoDB (academic/identity data) with CloudWatch/SNS/Secrets Manager for monitoring and security. CI/CD is implemented through GitLab Runner combined with AWS SAM CLI.\nBenefits and Return on Investment (ROI)\nAccelerated development and deployment: Automated CI/CD with GitLab Runner and CloudFormation reduces feature release time from days to hours. Optimized operational costs: Serverless architecture (Lambda, DynamoDB, API Gateway) charges only per request, saving 40‚Äì60% of costs compared to traditional EC2. Comprehensive security: Secrets Manager and Cognito combined with IAM provide robust protection for sensitive data and strict access control. High access performance: CloudFront CDN accelerates access and reduces latency by 50‚Äì70%. Flexible scalability: The system automatically scales according to traffic without manual intervention. Proactive monitoring: CloudWatch + SNS provides real-time alerts, helping the technical team respond promptly to incidents. 3. Solution Architecture Detailed Description User Request Flow\nUsers open a browser and access the application through a domain. Amazon CloudFront receives the request: Checks the cache to return static content (HTML/CSS/JS) from S3 if available. If content is not in cache, CloudFront fetches from S3 and returns it to the user with low latency. Users receive the frontend and interact with the UI, generating API requests. Authentication \u0026amp; API Handling\nWhen users log in: Amazon Cognito authenticates login information (username/password or OAuth). Cognito generates a JWT token and returns it to the client. The frontend sends API requests with the JWT token to API Gateway. API Gateway performs: Verifies the JWT token with Cognito. If the token is valid, the request is forwarded to the corresponding Lambda function. If the token is invalid, API Gateway returns a 401 Unauthorized error. Lambda Processing \u0026amp; Data Access\nAWS Lambda executes business logic: Manages students, attendance, course registration, updates results, schedules‚Ä¶ When accessing sensitive information (API keys, DB passwords), Lambda calls AWS Secrets Manager. Lambda reads/writes data to Amazon DynamoDB: DynamoDB stores data in a NoSQL model, optimized for read/write, automatically scaling when traffic increases. Supports queries using primary key (PK) or secondary index (GSI). Lambda logs to CloudWatch Logs, including requests, errors, and execution metrics. Security \u0026amp; Access Control\nAmazon Cognito: authenticates users and manages sessions, supporting OAuth/Google Sign-In. IAM Roles \u0026amp; Policies: control access permissions between AWS services (Lambda, DynamoDB, S3). API Gateway Authorization: validates JWT tokens from Cognito before allowing Lambda access. Secrets Manager: protects sensitive information (API keys, database credentials), enabling secure Lambda access. Monitoring \u0026amp; Alerts\nCloudWatch Logs collects logs from Lambda and API Gateway. Metrics \u0026amp; Alarms: Creates metrics from logs (CPU, error rate, latency, request count). Configures CloudWatch Alarms to trigger alerts when thresholds are exceeded. Amazon SNS sends real-time alerts to the operations team via email or HTTP/SMS endpoints. Combines CloudTrail + CloudWatch for auditing API actions and overall security. CI/CD \u0026amp; Deployment\nGitLab: stores source code and manages version control. GitLab Runner: Automatically triggers on code push or merge request. Runs CI/CD pipeline with stages: test, build, deploy. Installs dependencies and runs unit tests. Builds artifacts (ZIP package for Lambda). AWS SAM CLI / CloudFormation: GitLab Runner uses AWS SAM CLI for deployment. Deploys or updates the entire AWS infrastructure (API Gateway, Lambda, DynamoDB, S3, IAM Role). Ensures infrastructure follows IaC model, consistent across Dev/Prod environments. Automated CI/CD reduces errors and shortens deployment time. Summary\nRequest Path: User ‚Üí CloudFront ‚Üí API Gateway ‚Üí Lambda (via Cognito Auth) ‚Üí DynamoDB ‚Üí Lambda ‚Üí API Gateway ‚Üí CloudFront ‚Üí User. Security Path: Cognito Authentication ‚Üí API Gateway Authorization ‚Üí IAM Policies ‚Üí Secrets Manager. Monitoring: CloudWatch Logs \u0026amp; Metrics ‚Üí Alarms ‚Üí SNS. CI/CD Path: GitLab ‚Üí GitLab Runner ‚Üí AWS SAM CLI ‚Üí CloudFormation ‚Üí AWS Resources. AWS Services Used Services Description Frontend \u0026amp; CDN CloudFront, S3 Content distribution, static storage Backend \u0026amp; Logic API Gateway, Lambda, DynamoDB, Secrets Manager, Cognito Serverless logic, data, authentication Monitoring CloudWatch Logs, CloudWatch Alarms, CloudWatch Metrics, SNS Monitoring, alerts, metrics collection CI/CD \u0026amp; IaC CloudFormation, SAM Automated deployment and infrastructure management (with GitLab Runner) 4. Technical Implementation Deployment Stages\nDevelopment Stage Complete business logic and main flow for Lambda functions. Write template.yaml file describing resources: API Gateway, Lambda Functions, DynamoDB, Cognito. Use AWS SAM CLI to deploy code and template.yaml to LocalStack for local testing. Deployment Stage: Use AWS SAM CLI to deploy code and template.yaml to the real AWS environment. Configure GitLab CI/CD with GitLab Runner to automate the build and deployment process. Technical Requirements\nHave an AWS account using Free Tier to deploy and use resources normally. The template.yaml file must be correctly configured to fully describe all services. The system must have an automatic rollback mechanism in case of deployment failure. 5. Roadmap \u0026amp; Deployment Milestones Pre-Internship (Week 0): Learn AWS services to prepare for the project. Survey, analyze requirements and related departments of real centers (HR, Training, Admissions). Internship (Week 1-12): Week 1‚Äì3: System design, UI design, overall architecture, and prepare documentation (proposal, diagrams, SAM template). Week 4‚Äì8: Develop core modules (student management, instructor management, class management, user authentication). Local testing using LocalStack. Week 9‚Äì11: Integrate modules, finalize CI/CD pipeline, deploy the system to the real AWS environment. Week 12: End-to-end testing, evaluate results, complete report, and propose future development directions. Post-Internship (Expansion Phase ‚Äì Week 12 onwards): Upgrade the system, optimize performance, and integrate AI (personalized learning analytics) and IoT (smart classroom management). 6. Budget Estimation You can view costs on AWS Pricing Calculator\nOr download the budget estimate files pdf | csv | json\nInfrastructure Costs\nS3 Standard: 0.32 USD/month (10 GB, 5,000 PUT requests, 100,000 GET requests). CloudFront: 1.33 USD/month (10 GB, Data transfer out to origin 0.1 GB, Number of HTTPS requests 100,000). Amazon API Gateway: 0.38 USD/month (300,000 requests). AWS Lambda Function - Include Free Tier: 0.00 USD/month (400,000 requests, 512 MB storage). Amazon DynamoDB: 0.62 USD/month (Data storage 2 GB, 50,000 Writes, 200,000 Reads) Amazon Cognito Lite Tier: 0.00 USD/month (500 MAUs) Amazon CloudWatch: 2.10 USD/month (3 custom metrics, 1 GB logs, 1 dashboard, 2 alarms) AWS Secrets Manager: 0.40 USD/month (1 secret) Amazon SNS: 0.00 USD/month (1M requests, 1M Lambda deliveries) AWS CloudFormation: 0.00 USD/month GitLab Runner: 0.00 USD/month (self-hosted or GitLab Free Tier) Total: 5.15 USD/month, 61.80 USD/12 months\n7. Risk Assessment Risk Matrix\nAWS configuration errors (IAM, Lambda, API Gateway, Cognito): High impact, medium probability Exceeding AWS Free Tier limits: Medium impact, low probability. Data loss on S3/DynamoDB: High impact, low probability. Integration errors between AWS services: Medium impact, low probability. External attacks (SQL injection, XSS, unauthorized access): High impact, low to medium probability Mitigation Strategies\nAWS Configuration: Carefully check template.yaml, deploy on LocalStack before production deployment. Free Tier Exceeding: Monitor costs regularly, set up Billing Alerts, optimize resources. Data Loss: Enable S3 Versioning, periodically backup DynamoDB data. Service Integration Errors: Ensure services operate in the same Region, verify IAM Roles and cross-service access permissions. Application Security: Validate input at Lambda layer, use Cognito for strict authentication, configure CORS properly on API Gateway, apply principle of least privilege for IAM roles, enable CloudWatch and API Gateway logging for auditing. Contingency Plan\nDeployment failures: Rollback using AWS SAM CLI or restore previous Lambda version through CloudFormation stack. Budget overrun: Pause non-essential services, optimize architecture and resource usage. Security incidents: Review CloudWatch logs, disable compromised users/tokens via Cognito, review IAM permissions, isolate affected Lambda functions. 8. Expected Outcomes The teaching center management system is successfully deployed on AWS Serverless, ensuring stable, secure, and scalable operations. Optimize operational costs by leveraging AWS Free Tier and serverless architecture, reducing initial infrastructure investment. Ensure high access performance, fast response time, and flexible scalability. Guarantee data safety with backup, versioning, and strict access control mechanisms. CI/CD integration automates deployment, testing, and rollback, ensuring efficient and reliable development processes. "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.2-prerequisites/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "Environment Requirements To complete this workshop, we need to prepare the following.\nAWS Account:\nAn AWS account (Free Tier is acceptable) An IAM user with permissions for the following services: AWS CloudFormation AWS Lambda Amazon API Gateway Amazon S3 Amazon DynamoDB Amazon Cognito CloudWatch Logs Create AWS Access Key (Assuming you already have an IAM User)\nSign in to AWS Find and select the IAM service Select Users in the left menu to view the list of IAM Users Find and select the user you previously created Go to the Security Credentials section Choose Create Access Key to generate a new access key Choose CLI and check Confirmation Keep the Access Key and Secret Key secure and do not share them with others. AWS CLI:\nInstallation guide\nCheck version\naws --version AWS SAM CLI:\nInstallation guide\nCheck version\nsam --version AWS CLI Configuration Configure credentials\naws configure Enter the Access Key and Secret Key:\nNote: Do not expose your Access Key/Secret Key\nClone Workshop Repo Clone from Git\ngit clone https://github.com/Tai-isme/fcj-workshop-s3-notifications cd fcj-workshop-s3-notifications Download ZIP\nfcj-workshop-s3-notifications.zip Project directory structure\nüì¶fcj-workshop-s3-notifications\r‚î£ üìÇexcel-import-frontend\r‚îÉ ‚î£ üìÇsrc\r‚îÉ ‚î£ üìúpackage.json\r‚îÉ ‚îó üìúvite.config.js\r‚îó üìÇexcel-import-workshop\r‚îÉ ‚î£ üìÇsrc\r‚îÉ ‚î£ üìúpom.xml\r‚îÉ ‚îó üìútemplate.yaml "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.4-deploy-backend/5.4.2-sam-deploy/",
	"title": "SAM Deploy",
	"tags": [],
	"description": "",
	"content": "Deploy with AWS SAM SAM CLI will package the code and deploy the entire infrastructure to AWS through CloudFormation.\nDeploy There are 2 ways to deploy the application using SAM:\nMethod 1 ‚Äî Create S3 Bucket Manually aws s3 mb s3://demo-workshop-be-\u0026lt;YOUR-ID-ACCOUNT\u0026gt; --region ap-southeast-1 sam deploy --s3-bucket \u0026lt;bucket-name-just-created\u0026gt; --stack-name \u0026lt;stack-name\u0026gt; --region ap-southeast-1 Method 2 ‚Äî Use --guided (recommended for first time) sam deploy --guided To keep it simple and avoid creating a bucket manually, we will use Method 2. --guided will ask for parameters (stack name, region, permissions, etc.) and save the configuration to samconfig.toml so next time you only need to run sam deploy.\nSAM will ask some guided questions When running sam deploy --guided, SAM will ask for some parameters to configure (for example):\nStack Name [excel-import-workshop]: Enter the stack name (or leave blank to use the current project name). AWS Region [ap-southeast-1]: Enter Region (or leave blank to use ap-southeast-1). Parameter Environment [dev]: Enter environment (default is dev if left blank). Confirm changes before deploy [y/N]: Enter y to review changes before deploying. Allow SAM CLI IAM role creation [y/N]: Enter y to allow SAM to create IAM roles for Lambda. Disable rollback [y/N]: Enter n to enable rollback if deployment fails (recommended n). Save arguments to configuration file [y/N]: Enter y to save the configuration to samconfig.toml (so next time you only need to run sam deploy). If you choose y, SAM will continue asking for the configuration file name (default is samconfig.toml) and environment:\nSAM configuration file [samconfig.toml]: samconfig.toml SAM configuration environment [default]: default In this example I chose n to not save the configuration.\nDeployment Process Preparing CloudFormation\nCloudFormation Change Set\nSAM will display the list of resources to be created:\nConfirm deploy: Enter: y to deploy\nCloudFormation Execution\nCloudFormation Execution Success\nIf you have deployed once with --guided and chose y at save configuration file: Then the next deployment only needs:\nsam build sam deploy SAM will read the config from samconfig.toml.\nVerify Created Resources Check CloudFormation Stack Open CloudFormation Console Find stack excel-import-workshop Stack status should be: CREATE_COMPLETE Check Lambda Open Lambda Console Select Functions Check API Gateway Open API Gateway Console Select api Check S3 Bucket Open S3 Console Select Bucket workshop-excel-imports Check User pool Open Cognito Console Select ExcelWorkshopUsers All resources have been deployed successfully and completely.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - How INRIX accelerates transportation planning with Amazon Bedrock This blog introduces how INRIX leverages Amazon Bedrock and Amazon Nova Canvas to apply generative AI in transportation planning, helping analyze a 50-petabyte data warehouse to identify high-risk locations and automatically propose appropriate safety measures. By integrating Nova Canvas‚Äôs image generation and in-painting capabilities, the system can visualize solutions directly on real-world images, shortening design time from several weeks to just a few days, thereby accelerating the planning process, reducing costs, and improving urban traffic safety efficiency.\nBlog 2 - Yggdrasil Gaming boosts speed, resilience, and innovation with GOStack and AWS This blog describes how Yggdrasil Gaming, a global online casino game developer, partnered with GOStack and AWS to modernize its entire technology infrastructure, paving the way for an AI-driven and no-code game development future. By migrating to a modern cloud architecture with services such as Amazon EKS, Aurora, EC2 GPU, CloudFront, and Bedrock, Yggdrasil reduced costs by 30%, increased processing capacity sixfold, and boosted uptime to 99.5%. The company also launched its Game-in-a-Box platform, enabling no-code game creation with AI-assisted design, localization, and testing, significantly shortening product launch times. Additionally, by leveraging Redshift, S3, and QuickSight, Yggdrasil can analyze player data in real time to optimize user experience and development strategy. With AWS, Yggdrasil has achieved exceptional speed, scalability, and innovation, paving the way for a more flexible and creative era in game development.\nBlog 3 - Qwen3 family of reasoning models now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart This blog announces the launch of the Qwen3 large language model series on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart, enabling users to easily and securely deploy, experiment with, and scale generative AI applications on AWS. Qwen3 is the latest generation in the Qwen family, supporting both dense and mixture-of-experts (MoE) models. It stands out for its superior reasoning capabilities, strong instruction adherence, multilingual support (over 100 languages), and flexible integration between reasoning and non-reasoning modes.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.3-architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "System Architecture Overview The Excel Import system is designed as a Serverless Event-Driven Architecture, leveraging AWS managed services to reduce operational overhead and optimize costs.\nDetails Endpoints:\nPOST /register (RegisterFunction) POST /confirm (ConfirmFunction) POST /login (LoginFunction) POST /logout (LogoutFunction) POST /upload-url (GenerateUploadUrlFunction) GET /import/jobs (ListImportJobsFunction) GET /jobs/{jobId} (GetJobStatusFunction) Core Processing Function\nProcess Flow:\nGet file from S3 (event.Records[0].s3) Parse Excel using Apache POI Validate each row (email format, required fields) Batch write to DynamoDB (25 items/batch) Update ImportJob status \u0026amp; statistics Output: Updated ImportJob record Tables: StudentsTable, CoursesTable, ImportJobsTable Amazon S3 (File Storage): Storage for user-imported files. "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 - 16:40, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 - 16:30, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Workshop \u0026ldquo;Data science on AWS\u0026rdquo; ‚Äì Unlock the Power of Data with Cloud Computing\nDate \u0026amp; Time: 9:30 - 11:45, October 16, 2025\nLocation: Hall A - FPTU HCM City\nRole: Attendee\nEvent 4 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 - 12:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 8:30 - 17:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 8:30 - 12:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.4-deploy-backend/",
	"title": "Deploy Backend",
	"tags": [],
	"description": "",
	"content": "Deploy Backend with AWS SAM In this section, we will use a SAM Template to deploy the entire infrastructure to AWS.\nSteps Build with Maven (compile Java)\nBuild with SAM (package Lambda)\nDeploy with SAM (create CloudFormation stack)\nVerify Resources (check on the AWS Console)\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.5-deploy-frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Update config.js File Step 1: Get API Gateway Endpoint\nOpen CloudFormation Console Find stack excel-import-workshop Select Output tab Copy value of ApiUrl Step 2: Change Configuration\nGo to /excel-import-frontend/ directory Go to /excel-import-frontend/src directory Open config.js file Replace APP_API_URL with the value of ApiUrl copied earlier Save it Step 3: Build Frontend and Create S3 Bucket to Host Frontend\nNavigate to the Frontend directory and run the following commands in order:\ncd ./excel-import-frontend/ npm install npm run build aws s3 mb s3://workshop-frontend-\u0026lt;ACCOUNT-ID\u0026gt; --region ap-southeast-1 After the build is complete, the dist folder will be created.\nStep 4: Host Frontend on S3 Bucket\nGo to S3 Bucket Console Select Bucket workshop-frontend-\u0026lt;ACCOUNT-ID\u0026gt; just created Open dist folder (created from npm run build command in Step 3), press CTRL + A to copy all files and folders. Drag all files and folders copied into the Upload section of the Bucket Click Upload, after upload success click Close Go to Permissions tab Click Edit Block public access (bucket settings) Uncheck Block public access Go to Object Ownership change to ACLs enabled then Save Select Object tab select all files and folders click Action and select Make Public and Close. Click select Index.html copy Object URL in the new tab to access the website. "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Excel-to-DynamoDB on AWS using S3 Notifications Overview In this workshop, we will build a complete serverless application to import data from Excel files to DynamoDB using AWS services:\nAWS Lambda to handle backend logic Amazon S3 to store Excel files S3 Event Notifications to automatically trigger Lambda when new files arrive Amazon DynamoDB to store data from excel Amazon Cognito for user authentication API Gateway to expose REST APIs We will perform:\nDeploy serverless application using AWS SAM Configure S3 Events to trigger Lambda Parse Excel files in Lambda and save to DynamoDB Integrate Cognito authentication Monitor import job progress System Architecture This workshop implements an event-driven serverless architecture with the following components:\nAPI Layer:\nAPI Gateway with Cognito Authorizer REST endpoints for authentication and import operations Processing Layer:\nLambda functions handling business logic S3 Event Notifications trigger processing Apache POI library to parse Excel files Storage Layer:\nS3 bucket for file uploads DynamoDB tables for data from excel files and import jobs Contents Workshop Overview Environment Setup System Architecture Deploy Backend (AWS SAM) Deploy Frontend (React) Test Application Cleanup Resources "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.6-test-application/",
	"title": "Test Application",
	"tags": [],
	"description": "",
	"content": "Test Application In this section we test the entire workflow: user registration, authentication, uploading sample files and monitoring import status.\nSign up and authentication On the login screen, click Sign up. Fill in the information and click Sign up to create an account. Enter the Code sent to your email and click Verify Email. After verification, log in with the Email and Password you just created. Download sample file Download the sample import file: import-template.xlsx Upload file and import Go to the file upload function, select the sample file you just downloaded and upload it. After uploading, click Upload \u0026amp; Import to start the import process. Monitor progress The file will be uploaded to S3 Bucket and trigger Lambda to import data. Import status: Processing ‚Üí if no error, it will change to Completed. Processing ‚Üí if there is an error, it will change to Failed and the system will automatically rollback. Verify data after import Check file on S3\nRun the following command to check the file you just uploaded:\naws s3 ls s3://workshop-excel-imports-\u0026lt;ACCOUNT-ID\u0026gt; --recursive Check table data\nGo to DynamoDB Console ‚Üí Explore items, select each table to view the data after import. "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd from 08/09/2025 to 14/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI worked with my team to develop a project focused on implementing a Learning Management System (LMS), through which I improved my skills in JavaScript programming, working with AWS, requirements analysis \u0026amp; processing, technical documentation \u0026amp; reporting, and communication \u0026amp; teamwork.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚òê ‚úÖ ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/5-workshop/5.7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "1) Delete S3 Buckets Content\n# Delete all objects in the bucket (empty) aws s3 rm s3://workshop-excel-imports-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; --recursive aws s3 rm s3://workshop-frontend-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; --recursive 2) Delete S3 Buckets\nAfter the bucket is empty, delete the bucket using the command:\naws s3 rb s3://workshop-excel-imports-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; aws s3 rb s3://workshop-frontend-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; 3) Handle SamCliSourceBucket created by SAM CLI\nSamCliSourceBucket is a temporary bucket created by AWS SAM CLI to upload source code when running sam deploy. If this bucket has Bucket Versioning enabled, you must manually delete the versioned objects before deleting the bucket. Open AWS Console -\u0026gt; S3 -\u0026gt; select SamCliSourceBucket (example name: aws-sam-cli-managed-default-...). Go to Properties tab -\u0026gt; Bucket Versioning. If Versioning = Enabled, delete the object versions (or temporarily disable versioning and delete each version) then empty the bucket. After the bucket has no objects (all versions), select Delete bucket and confirm. 4) Delete CloudFormation stack (SAM delete)\ncd ./excel-import-workshop/ sam delete --stack-name excel-import-workshop --region us-east-1 --no-prompts "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment at FCJ is very friendly and open. Team members are always willing to support me whenever I face difficulties, even outside working hours. The workspace is clean and organized, which helps me stay focused. However, I think the company could add more bonding activities or internal events to help members understand each other better.\n2. Support from Mentor / Team Admin\nMy mentor provided detailed guidance, explained clearly whenever I had questions, and always encouraged me to ask for clarification. Instead of giving direct answers, my mentor allowed me to explore and solve the problem on my own, which helped strengthen my analytical and problem-solving skills. The admin team supported me with all necessary procedures, documents, and provided a smooth environment for work and learning.\n3. Relevance of Work to Academic Major\nThe tasks assigned were aligned with the knowledge I learned at university, while also introducing me to new areas I had not encountered before. This helped me strengthen my foundational skills while gaining practical industry experience.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I acquired valuable skills such as:\nUsing project management tools Effective teamwork Professional communication in a corporate environment Problem-solving and analytical thinking My mentor also shared many real-world insights that helped me better shape my career direction. 5. Company Culture \u0026amp; Team Spirit\nThe company culture is highly positive. Everyone respects each other, works seriously but maintains a friendly atmosphere. During high-pressure periods, team members support each other regardless of position. This helped me feel like part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides reasonable internship allowances and offers flexible working hours when I need to adjust my schedule. Being able to join internal training sessions is also a major benefit that supports learning.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with the strong support from my mentor and the team. Besides technical guidance, they shared valuable thinking approaches and career development insights, helping me grow both professionally and personally.\nWhat do you think the company should improve for future interns?\nSome improvements that may benefit future interns include:\nMore internal training sessions on specific topics Regular team-building activities More detailed onboarding documentation for new interns If recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would recommend FCJ to my friends because:\nFriendly and positive environment Supportive and dedicated mentors High opportunity for practical learning Clear and well-defined tasks suitable for students Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nIncrease cross-team knowledge-sharing activities Organize monthly mini-workshops for interns to learn new skills Provide more detailed project documentation and guidance Would you like to continue this program in the future?\nIf possible, I would like to continue participating in the program or transition to a collaborator/full-time role, as the working environment and development path are very suitable for me.\nAny other comments (free sharing):\nNo further suggestions, except that I hope the company continues maintaining the positive culture and supportive spirit it currently has.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Strengthen practical experience in automation, tagging, IAM, Lambda, and event-driven operations. Improve governance capabilities across multi-region workloads. Build end-to-end visibility and control through CloudTrail, Athena, and KMS integration. Tasks to be carried out this week: Day Task Start Date Completion Date 1 - Lab22-2.1 ‚Äì Create VPC - Lab22-2.2 ‚Äì Create Security Group - Lab22-2.3 ‚Äì Create EC2 Instance - Lab22-2.4 ‚Äì Incoming Webhooks Slack 03/11/2025 03/11/2025 2 - Lab22-3 ‚Äì Create Tag for Instance - Lab22-4 ‚Äì Create Role for Lambda - Lab22-5.1 ‚Äì Function Stop Instance - Lab22-5.2 ‚Äì Function Start Instance - Lab22-6 ‚Äì Check Result - Lab22-7 ‚Äì Cleanup 04/11/2025 04/11/2025 3 - Lab27-2.1.1 ‚Äì Create EC2 Instance with Tag - Lab27-2.1.2 ‚Äì Managing Tags - Lab27-2.1.3 ‚Äì Filter Resources by Tag - Lab27-2.2 ‚Äì Using Tags with CLI - Lab27-3 ‚Äì Create Resource Group - Lab27-4 ‚Äì Cleanup 05/11/2025 05/11/2025 4 - Lab28-2.1 ‚Äì Create IAM User - Lab28-3 ‚Äì Create IAM Policy - Lab28-4 ‚Äì Create IAM Role 06/11/2025 06/11/2025 5 - Lab28-5.1 ‚Äì Switch Roles - Lab28-5.2.1 ‚Äì Access EC2 in Tokyo - Lab28-5.2.2 ‚Äì Access EC2 in North Virginia - Lab28-5.2.3 ‚Äì Create EC2 with missing/qualified tags - Lab28-5.2.4 ‚Äì Edit Resource Tag - Lab28-5.2.5 ‚Äì Policy Check - Lab28-6 ‚Äì Cleanup 07/11/2025 07/11/2025 6 - Lab30-3 ‚Äì Create Restriction Policy - Lab30-4 ‚Äì Create Limited IAM User - Lab30-5 ‚Äì Test IAM User Limits - Lab30-6 ‚Äì Cleanup 08/11/2025 08/11/2025 7 - Lab33-2.1 ‚Äì Create Policy \u0026amp; Role - Lab33-2.2 ‚Äì Create Group \u0026amp; User - Lab33-3 ‚Äì Create KMS - Lab33-4.1 ‚Äì Create S3 Bucket - Lab33-4.2 ‚Äì Upload Data to S3 - Lab33-5.1 ‚Äì Create CloudTrail - Lab33-5.2 ‚Äì Logging to CloudTrail - Lab33-5.3 ‚Äì Create Athena - Lab33-5.4 ‚Äì Query with Athena - Lab33-6 ‚Äì Test \u0026amp; Share Encrypted S3 Data - Lab33-7 ‚Äì Cleanup 09/11/2025 09/11/2025 Week 9 Achievements: Strengthened foundational infrastructure skills by creating a VPC, Security Group, and EC2 instance, reinforcing cloud networking and secure deployment principles.\nIntegrated cloud automation into communication workflows by configuring Slack Incoming Webhooks, enabling automated alerts ‚Äî a key DevOps monitoring capability.\nEnhanced tagging and automation proficiency through:\nCreating and managing instance tags Creating Lambda execution roles Writing Lambda functions to start/stop EC2 instances Validating automation results Cleaning up resources to maintain a tidy environment Developed strong resource governance skills with AWS Tags:\nCreating, editing, filtering, and managing tags at scale Using tags via the CLI Creating Resource Groups for structured cloud organization Reinforced IAM expertise by creating IAM Users, Policies, and Roles, following least-privilege principles to secure identity workflows.\nGained real-world IAM scenario experience:\nSwitching roles across accounts Accessing EC2 in Tokyo and North Virginia Testing policy behavior with missing/restricted tags Editing resource tags Running policy simulations Performing complete environment cleanup Strengthened security governance by creating Restriction Policies, limited-permission IAM users, and validating enforcement of access controls.\nMastered an end-to-end security visibility and data protection pipeline by completing advanced labs:\nCreating IAM policies, roles, groups, and users Deploying and managing KMS keys Creating S3 buckets and uploading encrypted data Enabling CloudTrail for API auditing Querying CloudTrail logs via Athena Testing secure data sharing with KMS-encrypted S3 objects Cleaning up all resources for cost efficiency "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Strengthen database fundamentals across relational, non-relational, and cloud-native architectures. Build hands-on experience with RDS, Aurora, Redshift, and ElastiCache. Practice real-world database deployment, backup, restore, and migration workflows. Tasks to be carried out this week: Day Task Start Date Completion Date 1 - Module 06-01 ‚Äì Database Concepts Review 10/11/2025 10/11/2025 2 - Module 06-02 ‚Äì Amazon RDS \u0026amp; Amazon Aurora 11/11/2025 11/11/2025 3 - Module 06-03 ‚Äì Redshift \u0026amp; ElastiCache 12/11/2025 12/11/2025 4 - Lab05-2.1 ‚Äì Create VPC - Lab05-2.2 ‚Äì Create EC2 Security Group - Lab05-2.3 ‚Äì Create RDS Security Group - Lab05-2.4 ‚Äì Create DB Subnet Group 13/11/2025 13/11/2025 5 - Lab05-3 ‚Äì Create EC2 Instance - Lab05-4 ‚Äì Create RDS Database Instance 14/11/2025 14/11/2025 6 - Lab05-5 ‚Äì Application Deployment - Lab05-6 ‚Äì Backup \u0026amp; Restore 15/11/2025 15/11/2025 7 - Lab05-7 ‚Äì Clean Up Resources - Lab43-01 ‚Üí Lab43-06 (EC2 Connect, Oracle/MSSQL Config) - Lab43-07 ‚Üí Lab43-17 (Schema Conversion, Migration Tasks, Events, Logs, Troubleshooting) 16/11/2025 16/11/2025 Week 10 Achievements: Strengthened database fundamentals through a structured review of relational vs. non-relational models, normalization, indexing, transactions, isolation levels, and high availability concepts essential to cloud database design.\nGained deep knowledge of Amazon RDS and Amazon Aurora, including:\nEngine families Storage models Multi-AZ deployment Read Replicas Automated backups Failover operations Aurora‚Äôs distributed storage built for performance and durability Expanded analytical and caching capabilities by learning:\nAmazon Redshift for data warehousing Amazon ElastiCache (Redis/Memcached) for in-memory caching and low-latency workloads Completed foundational database infrastructure labs:\nBuilt a VPC designed specifically for database environments Configured EC2 \u0026amp; RDS Security Groups with least-privilege rules Created a DB Subnet Group spanning multiple Availability Zones Successfully deployed an EC2 instance and created an RDS database instance, forming a complete application-to-database architecture inside a secure network.\nPerformed an end-to-end application deployment connecting EC2 ‚Üí RDS.\nPracticed essential database operations including backup and restore, verifying durability and recoverability.\nExecuted responsible cloud hygiene through full resource cleanup.\nImproved database migration expertise with an advanced, multi-stage lab involving:\nEC2 Connect for database administration Oracle \u0026amp; MSSQL configuration Schema conversion using AWS SCT Migration tasks using AWS DMS Event monitoring, log analysis, performance tuning, troubleshooting Gained practical, real-world experience in full database migration pipelines, including schema transformation, data replication, validation, and operational cleanup ‚Äî reflecting authentic enterprise scenarios.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Strengthen data engineering skills using S3, Kinesis Firehose, Glue, Athena, and QuickSight. Build and analyze NoSQL workloads using Amazon DynamoDB. Gain hands-on experience with serverless and event-driven architectures. Tasks to be carried out this week: Day Task Start Date Completion Date 1 - Lab35-3.1 ‚Äì Create S3 Bucket - Lab35-3.2 ‚Äì Creating a Delivery Stream 17/11/2025 17/11/2025 2 - Lab35-3.3 ‚Äì Create Sample Data - Lab35-4.1 ‚Äì Create Glue Crawler - Lab35-4.2 ‚Äì Data Check 18/11/2025 18/11/2025 3 - Lab35-5.1 ‚Äì Explain Code (VN version) - Lab35-5.2 ‚Äì S3 Store Output - Lab35-5.3 ‚Äì Session Connect Setup 19/11/2025 19/11/2025 4 - Lab35-6.1 ‚Äì Analysis with Athena - Lab35-6.2 ‚Äì Visualize with QuickSight - Lab35-7 ‚Äì Clean Up Resources 20/11/2025 20/11/2025 5 - Lab39-1 ‚Äì Hands-on DynamoDB - Lab39-2 ‚Äì Explore DynamoDB - Lab39-3 ‚Äì Explore DynamoDB Console 21/11/2025 21/11/2025 6 - Lab39-4 ‚Äì Back Up - Lab39-5 ‚Äì Clean Up - Lab39-6 ‚Äì Advanced Design Patterns for DynamoDB 22/11/2025 22/11/2025 7 - Lab39-7 ‚Äì Build \u0026amp; Deploy Global Serverless with DynamoDB - Lab39-8 ‚Äì Serverless Event-Driven Architecture with DynamoDB 23/11/2025 23/11/2025 Week 11 Achievements: Strengthened data engineering fundamentals by creating an S3 bucket and building a Kinesis Data Firehose delivery stream, gaining practical experience in real-time ingestion and durable storage pipelines.\nDeveloped practical skills in sample data generation, ingestion workflows, and downstream analytics.\nBuilt a Glue Crawler to classify data and populate the Glue Data Catalog, then validated schema accuracy through structured data checks.\nEnhanced understanding of ETL workflows by explaining lab code logic, configuring S3 output storage, and establishing proper session connectivity for analytics tasks.\nPerformed end-to-end analytics using Amazon Athena:\nQuerying data directly from S3 Optimizing query performance Validating insights from processed datasets Built visual dashboards using Amazon QuickSight to strengthen BI and visualization capabilities.\nPracticed responsible cloud operations by executing full resource cleanup.\nGained both foundational and advanced Amazon DynamoDB knowledge, including:\nTable creation Data exploration \u0026amp; CRUD operations NoSQL design considerations and workloads Completed DynamoDB backup and restore operations and learned advanced design patterns:\nSingle-table design Partition key optimization Scalability, cost efficiency, and performance tuning Built and deployed global serverless architectures using DynamoDB Global Tables, achieving multi-region replication and high availability.\nExtended serverless knowledge by integrating DynamoDB Streams with other AWS components, building reactive and scalable event-driven architectures.\n"
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Strengthen end-to-end database and analytics workflows. Develop multi-interface AWS operational skills. Complete a full modern data pipeline from ingestion to BI dashboarding. Tasks to be carried out this week: Day Task Start Date Completion Date 1 - Lab40-2.1 ‚Äì Preparing the Database - Lab40-2.2 ‚Äì Building a Database 24/11/2025 24/11/2025 2 - Lab40-3.1 ‚Äì Data in the Table - Lab40-3.2 ‚Äì Cost - Lab40-3.3 ‚Äì Tagging \u0026amp; Cost Allocation 25/11/2025 25/11/2025 3 - Lab40-3.4 ‚Äì Usage - Lab40-3.5 ‚Äì Additional Result Query - Lab40-4 ‚Äì Clean Up Resources 26/11/2025 26/11/2025 4 - Lab60-1 ‚Äì CloudShell - Lab60-2 ‚Äì Console - Lab60-3 ‚Äì SDK 27/11/2025 27/11/2025 5 - Lab70-1.1 ‚Äì Create Cloud9 Instance - Lab70-1.2 ‚Äì Download Dataset - Lab70-1.3 ‚Äì Upload Dataset to S3 28/11/2025 28/11/2025 6 - Lab70-2.1 ‚Äì Setting Up DataBrew - Lab70-2.2 ‚Äì Data Profiling - Lab70-2.3 ‚Äì Clean \u0026amp; Transform Data 29/11/2025 29/11/2025 7 - Lab72-2 ‚Üí 72-13 ‚Äì Full Data Lifecycle - Lab73-3 ‚Üí 73-5 ‚Äì Build, Improve \u0026amp; Create Interactive Dashboard 30/11/2025 30/11/2025 Week 12 Achievements: Strengthened database operational skills by preparing and building a cloud-hosted database, designing table structures, validating relational schemas, and inserting sample data to ensure correctness and integrity.\nDeveloped practical cloud financial awareness through hands-on cost analysis‚Äîunderstanding how storage, compute, and queries influence overall cloud spending.\nPracticed tagging and cost allocation to improve tracking and accountability across environments.\nPerformed additional usage analysis and advanced query operations to extract meaningful insights before executing complete environment cleanup.\nGained multi-interface AWS experience through:\nCloudShell (command line) AWS Console (graphical interface) AWS SDK (programmatic access)\nStrengthened ability to manage cloud resources across different operational layers. Built a working Cloud9 development environment, downloaded datasets, and uploaded them into Amazon S3‚Äîforming the foundation for data transformation and analytics.\nEnhanced data engineering capabilities with AWS Glue DataBrew:\nData Profiling Data Cleaning Data Transformation Completed a comprehensive, end-to-end modern data lifecycle through Lab72, covering:\nIngest \u0026amp; Store\nCollected and stored raw data in Amazon S3 Catalog\nClassified and indexed data using AWS Glue Transform\nInteractive Glue Jobs GUI-based Glue Jobs DataBrew transformations EMR distributed data processing Analyze\nQueried data with Athena Performed streaming analytics using Kinesis Data Analytics Visualize\nBuilt dashboards and insights with Amazon QuickSight Serve\nDelivered processed data through AWS Lambda Warehouse\nLoaded and organized data into Amazon Redshift Dashboarding\nBuilt, improved, and enhanced interactive BI dashboards Completed multiple dashboard iterations, strengthening skills in:\nVisualization design Insight communication Dashboard interactivity\nfor real-world analytical reporting. "
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/4-eventparticipated/4-6-event6/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kliii18.github.io/AWS---workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]